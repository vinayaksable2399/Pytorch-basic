{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty((1,2),dtype=torch.double)\n",
    "a=torch.ones((4,50))\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=torch.tensor([2,3,3,2])\n",
    "c=torch.tensor([1])\n",
    "b\n",
    "c.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 3. 3. 3. 3.]\n",
      "tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 2, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.new_ones(1)\n",
    "b.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "Central to all neural networks in PyTorch is the ```autograd``` package. Let’s first briefly visit this, and we will then go to training our first neural network.\n",
    "\n",
    "The autograd package provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ### Note \n",
    "\n",
    ">Our torch.Tensor constructor is overloaded to do the same thing as both ```torch.tensor``` and ```torch.empty```. We thought this overload would make code confusing, so we split ```torch.Tensor``` into ```torch.tensor``` and ```torch.empty```.\n",
    "\n",
    "So @yxchng yes, to some extent, ```torch.tensor``` works similarly to ```torch.Tensor``` (when you pass in data). @ProGamerGov no, neither should be more efficient than the other. It’s just that the ```torch.empty``` and ```torch.tensor``` have a nicer API than our legacy ```torch.Tensor``` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4013e-45, 0.0000e+00]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.Tensor(1,2) #similar to torch.empty()\n",
    "print(a)\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=torch.tensor([1,2]) # its convert list to tensor\n",
    "print(b)\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "**```torch.Tensor``` is the central class of the package.** If you set its attribute ```.requires_grad``` as ```True```, it starts to track all operations on it. When you finish your computation you can call ```.backward()``` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into ```.grad``` attribute.\n",
    "\n",
    "To stop a tensor from tracking history, you can call ```.detach()``` to detach it from the computation history, and to prevent future computation from being tracked.\n",
    "\n",
    ">**Important Note Only Tensors of floating point dtype can require gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn) ##no function is used here so return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x0000001044F8EF28>\n"
     ]
    }
   ],
   "source": [
    "y = x+2\n",
    "print(y)\n",
    "print(y.grad_fn) # add function use here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y*3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "a=x.detach()\n",
    "a.requires_grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **IMPORTANT -: grad can be implicitly created only for scalar outputs**\n",
    "\n",
    "for this problem error occure at ```z.backword()``` and ```y.backward()``` because they are not **_scalar_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) ## darivative of mean(y*y*3 where y=x+2 and x=torch.ones(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function ones:\n",
      "\n",
      "ones(...)\n",
      "    ones(*sizes, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "    \n",
      "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
      "    by the variable argument :attr:`sizes`.\n",
      "    \n",
      "    Args:\n",
      "        sizes (int...): a sequence of integers defining the shape of the output tensor.\n",
      "            Can be a variable number of arguments or a collection like a list or tuple.\n",
      "        out (Tensor, optional): the output tensor\n",
      "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "            Default: ``torch.strided``.\n",
      "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "            Default: if ``None``, uses the current device for the default tensor type\n",
      "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "        requires_grad (bool, optional): If autograd should record operations on the\n",
      "            returned tensor. Default: ``False``.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> torch.ones(2, 3)\n",
      "        tensor([[ 1.,  1.,  1.],\n",
      "                [ 1.,  1.,  1.]])\n",
      "    \n",
      "        >>> torch.ones(5)\n",
      "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.requires_grad_( ... )``` changes an existing Tensor’s ```requires_grad``` flag in-place. The input flag defaults to ```False``` if not given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000001044F92470>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "Let’s backprop now. Because out contains a single scalar, ```out.backward()``` is equivalent to ```out.backward(torch.tensor(1.))```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<AddBackward0>)\n",
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor(3.0,requires_grad=True)\n",
    "y=x+2*x\n",
    "print(y)\n",
    "y.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 562.7458,  695.3374, -876.3957], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000: ## y.data.norm() its save past value\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in this case y is no longer a scalar. ```torch.autograd``` could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to ```backward``` as argument:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3], dtype=torch.float ,requires_grad=True)\n",
    "\n",
    "y = x **2+2*x-19*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SubBackward0 object at 0x0000001044F925C0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.ones(x.size(), dtype=torch.float)\n",
    "y.backward(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-15., -13., -11.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on Tensors with ```.requires_grad=True``` by wrapping the code block in with ```torch.no_grad():```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _NEURAL NETWORKS_\n",
    "Neural networks can be constructed using the ```torch.nn``` package.\n",
    "\n",
    "Now that you had a glimpse of ```autograd```, ```nn``` depends on ```autograd``` to define models and differentiate them. An ```nn.Module``` contains layers, and a method ```forward(input)```that returns the output.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "<img src=\"mnist.png\" />\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "+ Define the neural network that has some learnable parameters (or weights)\n",
    "+ Iterate over a dataset of inputs\n",
    "+ Process input through the network\n",
    "+ Compute the loss (how far is the output from being correct)\n",
    "+ Propagate gradients back into the network’s parameters\n",
    "+ Update the weights of the network, typically using a simple update rule: ```weight = weight - learning_rate * gradient```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network\n",
    "Let’s define this network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base class for all neural network modules.\n",
    "\n",
    "    Your models should also subclass this class.\n",
    "\n",
    "    Modules can also contain other Modules, allowing to nest them in\n",
    "    a tree structure. You can assign the submodules as regular attributes::\n",
    "\n",
    "        import torch.nn as nn\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        class Model(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Model, self).__init__()\n",
    "                self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "                self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "\n",
    "            def forward(self, x):\n",
    "               x = F.relu(self.conv1(x))\n",
    "               return F.relu(self.conv2(x))\n",
    "\n",
    "    Submodules assigned in this way will be registered, and will have their\n",
    "    parameters converted too when you call :meth:`to`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3) #Applies a 2D convolution over an input signal composed of several input planes.\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)# we can't cahnge input because it's defence of past output\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 576])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(len(params))\n",
    "for i in range(len(params)):\n",
    "    print(params[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let try a random ```32x32``` input. Note: expected input size of this net (LeNet) is 32x32. To use this net on MNIST dataset, please resize the images from the dataset to 32x32.\n",
    "\n",
    "+ **how to find function for random input size?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "tensor([[-0.0821, -0.1117,  0.0056, -0.0974, -0.1437, -0.0574,  0.1433, -0.0515,\n",
      "         -0.0215, -0.0584]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input= torch.randn(1, 3, 32, 32)\n",
    "print(input.size())\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,32,5)\n",
    "        self.pool1=nn.MaxPool2d(3,3)\n",
    "        self.conv2=nn.Conv2d(32,64,5)\n",
    "        self.pool2=nn.MaxPool2d(3,3)\n",
    "        self.conv3=nn.Conv2d(64,128,5)\n",
    "        self.pool3=nn.MaxPool2d(3,3)\n",
    "        self.fc1=nn.Linear(1024,7)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "        \n",
    "mynet=Model()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mynet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([32, 3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params1 = list(mynet.parameters())\n",
    "print(len(params1))\n",
    "print(params1[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([32, 3, 5, 5])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 5, 5])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 5, 5])\n",
      "torch.Size([128])\n",
      "torch.Size([7, 1024])\n",
      "torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(len(params1))\n",
    "for i in range(len(params1)):\n",
    "    print(params1[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torch.nn``` only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample.\n",
    "\n",
    "For example, ```nn.Conv2d``` will take in a 4D Tensor of ```nSamples x nChannels x Height x Width```.\n",
    "\n",
    "If you have a single sample, just use ```input.unsqueeze(0)``` to add a fake batch dimension.\n",
    "\n",
    "Before proceeding further, let’s recap all the classes you’ve seen so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Recap:\n",
    "+ ```torch.Tensor``` - A multi-dimensional array with support for autograd operations like ```backward()```. Also holds the gradient w.r.t. the tensor.\n",
    "+ ```nn.Module``` - Neural network module. Convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "+ ```nn.Parameter``` - A kind of Tensor, that is automatically registered as a parameter when assigned as an attribute to a Module.\n",
    "+ ```autograd.Function``` - Implements forward and backward definitions of an autograd operation. Every Tensor operation creates at least a single Function node that connects to functions that created a Tensor and encodes its history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "**A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.**\n",
    "\n",
    "There are several different <a href='https://pytorch.org/docs/stable/nn.html'>loss functions</a> under the ```nn``` package . A simple loss is: ```nn.MSELoss``` which computes the mean-squared error between the input and the target.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n",
      "tensor(0.2938, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "print(input.size())\n",
    "print(output.size())\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "print(target.size())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000001045DA2898>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000001045C21C18>\n",
      "<AddmmBackward object at 0x0000001045DA2128>\n",
      "<AccumulateGrad object at 0x0000001045DA2358>\n",
      "<ReluBackward0 object at 0x0000001045DA2128>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[1][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "To backpropagate the error all we have to do is to ```loss.backward()```. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
    "\n",
    "Now we shall call ```loss.backward()```, and have a look at conv1’s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0019, -0.0028,  0.0012, -0.0047,  0.0059,  0.0049])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights\n",
    "The simplest update rule used in practice is the Stochastic Gradient Descent (SGD):\n",
    "\n",
    "```weight = weight - learning_rate * gradient```\n",
    "\n",
    "\n",
    "We can implement this using simple python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before Parameter containing:\n",
      "tensor([[[[-0.1795,  0.1671, -0.0335],\n",
      "          [-0.1615,  0.0440, -0.0687],\n",
      "          [ 0.1038, -0.0299,  0.1830]],\n",
      "\n",
      "         [[-0.1211, -0.1595,  0.0300],\n",
      "          [-0.0855, -0.1143,  0.0670],\n",
      "          [-0.1743,  0.0134,  0.0508]],\n",
      "\n",
      "         [[ 0.0591,  0.0806,  0.0808],\n",
      "          [ 0.1592,  0.0263, -0.1577],\n",
      "          [ 0.1801, -0.1753,  0.0699]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0851,  0.0206,  0.0950],\n",
      "          [ 0.1856, -0.0009, -0.1206],\n",
      "          [ 0.0598, -0.1784,  0.0076]],\n",
      "\n",
      "         [[-0.0934, -0.1366, -0.1231],\n",
      "          [ 0.1597,  0.0266, -0.1067],\n",
      "          [-0.0471,  0.0119, -0.0034]],\n",
      "\n",
      "         [[ 0.0839, -0.1834, -0.0896],\n",
      "          [ 0.1347,  0.1373,  0.1259],\n",
      "          [ 0.0038,  0.0637,  0.1184]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0725,  0.1482, -0.1079],\n",
      "          [ 0.1368,  0.1459, -0.0400],\n",
      "          [ 0.0528,  0.0672,  0.0128]],\n",
      "\n",
      "         [[-0.1850,  0.1906,  0.0801],\n",
      "          [ 0.0986, -0.1383, -0.1513],\n",
      "          [ 0.1001, -0.0854,  0.1627]],\n",
      "\n",
      "         [[-0.0745, -0.1579, -0.1731],\n",
      "          [ 0.0536,  0.1395,  0.1814],\n",
      "          [ 0.0134,  0.0573, -0.1738]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0553,  0.0690,  0.0721],\n",
      "          [-0.1179,  0.1857, -0.0792],\n",
      "          [-0.1535, -0.1361,  0.0512]],\n",
      "\n",
      "         [[-0.0953, -0.1508, -0.1260],\n",
      "          [-0.1024,  0.0204, -0.1665],\n",
      "          [-0.0438,  0.1765, -0.1594]],\n",
      "\n",
      "         [[ 0.0128,  0.1591,  0.0556],\n",
      "          [ 0.0169, -0.1726,  0.1889],\n",
      "          [-0.0426, -0.1777,  0.0559]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1085,  0.0428, -0.1148],\n",
      "          [-0.1871,  0.0444,  0.1587],\n",
      "          [ 0.1907,  0.1003, -0.0281]],\n",
      "\n",
      "         [[-0.1805,  0.1316, -0.0434],\n",
      "          [-0.1684,  0.1683, -0.0139],\n",
      "          [-0.0819,  0.0433, -0.1676]],\n",
      "\n",
      "         [[-0.0969, -0.0578, -0.1200],\n",
      "          [ 0.0849, -0.1436, -0.1391],\n",
      "          [-0.1032, -0.1002, -0.0175]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1881, -0.0697,  0.1659],\n",
      "          [ 0.1335, -0.1744, -0.1487],\n",
      "          [ 0.0742,  0.0303, -0.1398]],\n",
      "\n",
      "         [[-0.1090,  0.0497,  0.1720],\n",
      "          [-0.0976, -0.0005, -0.0011],\n",
      "          [ 0.0909, -0.0955,  0.0397]],\n",
      "\n",
      "         [[-0.0463, -0.1653,  0.1707],\n",
      "          [-0.1447, -0.0111,  0.0321],\n",
      "          [-0.0027,  0.1692,  0.1142]]]], requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([[[[-0.1795,  0.1671, -0.0335],\n",
      "          [-0.1615,  0.0440, -0.0687],\n",
      "          [ 0.1038, -0.0299,  0.1830]],\n",
      "\n",
      "         [[-0.1212, -0.1595,  0.0300],\n",
      "          [-0.0854, -0.1143,  0.0670],\n",
      "          [-0.1742,  0.0135,  0.0508]],\n",
      "\n",
      "         [[ 0.0591,  0.0806,  0.0808],\n",
      "          [ 0.1592,  0.0263, -0.1577],\n",
      "          [ 0.1801, -0.1753,  0.0699]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0851,  0.0207,  0.0951],\n",
      "          [ 0.1856, -0.0010, -0.1206],\n",
      "          [ 0.0598, -0.1784,  0.0076]],\n",
      "\n",
      "         [[-0.0933, -0.1365, -0.1232],\n",
      "          [ 0.1597,  0.0266, -0.1067],\n",
      "          [-0.0471,  0.0119, -0.0034]],\n",
      "\n",
      "         [[ 0.0839, -0.1834, -0.0896],\n",
      "          [ 0.1348,  0.1372,  0.1258],\n",
      "          [ 0.0038,  0.0636,  0.1185]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0725,  0.1483, -0.1080],\n",
      "          [ 0.1369,  0.1458, -0.0400],\n",
      "          [ 0.0528,  0.0672,  0.0127]],\n",
      "\n",
      "         [[-0.1850,  0.1905,  0.0802],\n",
      "          [ 0.0987, -0.1383, -0.1514],\n",
      "          [ 0.1001, -0.0854,  0.1627]],\n",
      "\n",
      "         [[-0.0745, -0.1578, -0.1732],\n",
      "          [ 0.0536,  0.1395,  0.1814],\n",
      "          [ 0.0134,  0.0572, -0.1738]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0553,  0.0691,  0.0720],\n",
      "          [-0.1178,  0.1857, -0.0791],\n",
      "          [-0.1536, -0.1361,  0.0512]],\n",
      "\n",
      "         [[-0.0952, -0.1507, -0.1261],\n",
      "          [-0.1024,  0.0205, -0.1666],\n",
      "          [-0.0438,  0.1764, -0.1595]],\n",
      "\n",
      "         [[ 0.0129,  0.1591,  0.0555],\n",
      "          [ 0.0169, -0.1726,  0.1889],\n",
      "          [-0.0426, -0.1777,  0.0558]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1085,  0.0428, -0.1149],\n",
      "          [-0.1870,  0.0443,  0.1586],\n",
      "          [ 0.1906,  0.1004, -0.0281]],\n",
      "\n",
      "         [[-0.1805,  0.1316, -0.0434],\n",
      "          [-0.1684,  0.1684, -0.0139],\n",
      "          [-0.0818,  0.0433, -0.1676]],\n",
      "\n",
      "         [[-0.0969, -0.0578, -0.1199],\n",
      "          [ 0.0849, -0.1436, -0.1391],\n",
      "          [-0.1031, -0.1003, -0.0175]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1881, -0.0697,  0.1659],\n",
      "          [ 0.1335, -0.1744, -0.1486],\n",
      "          [ 0.0740,  0.0304, -0.1399]],\n",
      "\n",
      "         [[-0.1089,  0.0497,  0.1720],\n",
      "          [-0.0974, -0.0005, -0.0010],\n",
      "          [ 0.0907, -0.0954,  0.0396]],\n",
      "\n",
      "         [[-0.0464, -0.1653,  0.1707],\n",
      "          [-0.1447, -0.0111,  0.0321],\n",
      "          [-0.0027,  0.1693,  0.1142]]]], requires_grad=True)\n",
      "before Parameter containing:\n",
      "tensor([-0.1335,  0.1607,  0.0369,  0.1280, -0.0598,  0.1513],\n",
      "       requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([-0.1335,  0.1607,  0.0368,  0.1280, -0.0599,  0.1513],\n",
      "       requires_grad=True)\n",
      "before Parameter containing:\n",
      "tensor([[[[-0.0551, -0.0107, -0.0201],\n",
      "          [ 0.1325, -0.0647, -0.0104],\n",
      "          [-0.0324,  0.0699,  0.0174]],\n",
      "\n",
      "         [[ 0.0037,  0.0720, -0.0699],\n",
      "          [-0.0332,  0.0648, -0.0979],\n",
      "          [ 0.1019, -0.0140,  0.0129]],\n",
      "\n",
      "         [[ 0.0032, -0.0138, -0.0610],\n",
      "          [ 0.0222, -0.1320, -0.0412],\n",
      "          [-0.1174, -0.0175, -0.0659]],\n",
      "\n",
      "         [[ 0.0743,  0.0351,  0.0096],\n",
      "          [-0.1138,  0.0353,  0.0930],\n",
      "          [ 0.0489, -0.0581, -0.0654]],\n",
      "\n",
      "         [[-0.1357,  0.0610,  0.0415],\n",
      "          [ 0.0988, -0.0274, -0.0571],\n",
      "          [ 0.0121,  0.0050, -0.0400]],\n",
      "\n",
      "         [[-0.1074, -0.0150,  0.0380],\n",
      "          [-0.0212, -0.1051,  0.1030],\n",
      "          [ 0.0267, -0.0362,  0.0681]]],\n",
      "\n",
      "\n",
      "        [[[-0.0788, -0.0655,  0.1233],\n",
      "          [ 0.1329, -0.0597,  0.0565],\n",
      "          [-0.1005, -0.0167, -0.0329]],\n",
      "\n",
      "         [[-0.0677, -0.1262,  0.0470],\n",
      "          [-0.0848, -0.0422, -0.0672],\n",
      "          [-0.0057, -0.0232,  0.0044]],\n",
      "\n",
      "         [[ 0.0621,  0.1002, -0.0686],\n",
      "          [ 0.0981, -0.0550, -0.0006],\n",
      "          [ 0.0191, -0.0286, -0.0180]],\n",
      "\n",
      "         [[ 0.0589, -0.1321, -0.1287],\n",
      "          [-0.0579,  0.0748,  0.0380],\n",
      "          [ 0.0214, -0.0323,  0.0987]],\n",
      "\n",
      "         [[ 0.1173,  0.0853, -0.0473],\n",
      "          [-0.1145,  0.0351, -0.1034],\n",
      "          [-0.0410,  0.1021, -0.0275]],\n",
      "\n",
      "         [[ 0.1006,  0.0712, -0.0453],\n",
      "          [ 0.1285,  0.1157,  0.1226],\n",
      "          [-0.0566, -0.0930, -0.1045]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0289,  0.1104, -0.1285],\n",
      "          [ 0.1083, -0.0223,  0.0795],\n",
      "          [ 0.0417,  0.0811,  0.0768]],\n",
      "\n",
      "         [[-0.0649, -0.0062, -0.0866],\n",
      "          [-0.0603, -0.0931,  0.0249],\n",
      "          [ 0.0687,  0.1186, -0.1080]],\n",
      "\n",
      "         [[-0.0413,  0.0590,  0.1145],\n",
      "          [ 0.0832, -0.0653, -0.0114],\n",
      "          [ 0.0681, -0.0464, -0.1046]],\n",
      "\n",
      "         [[ 0.0313,  0.1142,  0.1210],\n",
      "          [ 0.1290,  0.0272, -0.0976],\n",
      "          [-0.0491,  0.1132,  0.1090]],\n",
      "\n",
      "         [[ 0.0305,  0.1138,  0.0952],\n",
      "          [ 0.0970,  0.1345, -0.0685],\n",
      "          [ 0.0876, -0.0209,  0.0503]],\n",
      "\n",
      "         [[ 0.0125,  0.0963,  0.0883],\n",
      "          [-0.0552,  0.0655,  0.1223],\n",
      "          [-0.0163,  0.0039,  0.0639]]],\n",
      "\n",
      "\n",
      "        [[[-0.0427, -0.1051,  0.0858],\n",
      "          [-0.0356, -0.0560,  0.0471],\n",
      "          [ 0.0771, -0.0486, -0.1151]],\n",
      "\n",
      "         [[ 0.0758,  0.0195,  0.0957],\n",
      "          [-0.0278, -0.0463, -0.1244],\n",
      "          [-0.0753, -0.0969,  0.0036]],\n",
      "\n",
      "         [[ 0.0857,  0.0358, -0.0868],\n",
      "          [ 0.0366,  0.0236, -0.0208],\n",
      "          [-0.0187,  0.1345,  0.0381]],\n",
      "\n",
      "         [[ 0.1215,  0.0017, -0.0346],\n",
      "          [-0.1062,  0.0052,  0.0548],\n",
      "          [ 0.0330, -0.0294,  0.0707]],\n",
      "\n",
      "         [[ 0.1171,  0.0164,  0.0565],\n",
      "          [ 0.1013, -0.0071,  0.0359],\n",
      "          [ 0.0434,  0.0820,  0.1329]],\n",
      "\n",
      "         [[-0.0880,  0.0343, -0.0311],\n",
      "          [ 0.0950,  0.1100,  0.0558],\n",
      "          [ 0.0495,  0.0357,  0.0442]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0702,  0.0066,  0.0368],\n",
      "          [ 0.1151,  0.0528, -0.0671],\n",
      "          [ 0.0470, -0.1125,  0.0720]],\n",
      "\n",
      "         [[ 0.1033, -0.0245,  0.0256],\n",
      "          [ 0.0716, -0.0230,  0.0858],\n",
      "          [-0.1287, -0.0992, -0.0497]],\n",
      "\n",
      "         [[ 0.1287, -0.0247,  0.0190],\n",
      "          [ 0.0298, -0.0615, -0.0939],\n",
      "          [-0.0691, -0.0513, -0.0509]],\n",
      "\n",
      "         [[-0.0446, -0.0517,  0.0291],\n",
      "          [ 0.0540, -0.0253, -0.1129],\n",
      "          [ 0.0523,  0.0170,  0.1148]],\n",
      "\n",
      "         [[-0.1018,  0.0161,  0.1313],\n",
      "          [-0.0481,  0.0104,  0.0734],\n",
      "          [-0.1033, -0.0089,  0.0386]],\n",
      "\n",
      "         [[ 0.0218, -0.1003, -0.0265],\n",
      "          [-0.0076,  0.0110,  0.0556],\n",
      "          [-0.0136, -0.1223,  0.0525]]],\n",
      "\n",
      "\n",
      "        [[[-0.0863, -0.0424, -0.0410],\n",
      "          [-0.1162,  0.1217,  0.0150],\n",
      "          [ 0.0742, -0.1047, -0.1014]],\n",
      "\n",
      "         [[-0.0415,  0.0566,  0.0579],\n",
      "          [ 0.0750, -0.0073, -0.1082],\n",
      "          [ 0.0253, -0.0516, -0.0993]],\n",
      "\n",
      "         [[ 0.0297,  0.0098, -0.1132],\n",
      "          [-0.1075, -0.1024,  0.1245],\n",
      "          [ 0.1023,  0.0576, -0.0691]],\n",
      "\n",
      "         [[ 0.0557, -0.0107,  0.0583],\n",
      "          [-0.1168, -0.0673, -0.1309],\n",
      "          [-0.1082,  0.0041, -0.0282]],\n",
      "\n",
      "         [[ 0.0580,  0.1332,  0.0963],\n",
      "          [ 0.1250, -0.0683,  0.0148],\n",
      "          [-0.0625,  0.0251,  0.0501]],\n",
      "\n",
      "         [[ 0.0621,  0.0237,  0.0762],\n",
      "          [ 0.1242, -0.0485, -0.0369],\n",
      "          [-0.0415, -0.0958, -0.0814]]],\n",
      "\n",
      "\n",
      "        [[[-0.0968, -0.0259,  0.0349],\n",
      "          [-0.0731, -0.0851,  0.0678],\n",
      "          [-0.0227, -0.0317,  0.1169]],\n",
      "\n",
      "         [[ 0.0644, -0.0867,  0.0654],\n",
      "          [ 0.0628,  0.0912, -0.0285],\n",
      "          [ 0.0878,  0.0596,  0.0030]],\n",
      "\n",
      "         [[-0.0802,  0.1273, -0.0118],\n",
      "          [-0.0910, -0.0834, -0.0512],\n",
      "          [-0.0814, -0.0588,  0.1177]],\n",
      "\n",
      "         [[-0.1282,  0.1095, -0.0228],\n",
      "          [ 0.0073,  0.0087, -0.0476],\n",
      "          [ 0.1111, -0.0858, -0.0837]],\n",
      "\n",
      "         [[ 0.1358, -0.1054, -0.0156],\n",
      "          [-0.0783, -0.0707, -0.1145],\n",
      "          [ 0.0242, -0.0161, -0.0846]],\n",
      "\n",
      "         [[ 0.0056, -0.0160,  0.0331],\n",
      "          [-0.0343, -0.1351, -0.1077],\n",
      "          [ 0.1037,  0.0162,  0.0300]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0782,  0.1296, -0.1144],\n",
      "          [-0.1046,  0.0592, -0.0454],\n",
      "          [-0.1282,  0.0118,  0.0064]],\n",
      "\n",
      "         [[-0.0915, -0.0069,  0.0150],\n",
      "          [ 0.1212, -0.0816, -0.0529],\n",
      "          [ 0.0103,  0.0467,  0.0502]],\n",
      "\n",
      "         [[-0.0882, -0.0153,  0.0304],\n",
      "          [-0.0538,  0.0185,  0.0354],\n",
      "          [ 0.1217,  0.0199, -0.0122]],\n",
      "\n",
      "         [[ 0.1115,  0.0088,  0.0521],\n",
      "          [-0.0011,  0.0201, -0.1085],\n",
      "          [-0.0124, -0.1291, -0.0123]],\n",
      "\n",
      "         [[ 0.1259,  0.1255, -0.0919],\n",
      "          [ 0.1333,  0.1155,  0.0378],\n",
      "          [-0.0247,  0.0594,  0.1201]],\n",
      "\n",
      "         [[ 0.1056,  0.0822,  0.0333],\n",
      "          [ 0.0433, -0.0455, -0.0008],\n",
      "          [-0.1323,  0.1305,  0.0225]]],\n",
      "\n",
      "\n",
      "        [[[-0.1006,  0.0800,  0.0117],\n",
      "          [ 0.0029,  0.0823, -0.0203],\n",
      "          [ 0.0955, -0.0870, -0.0383]],\n",
      "\n",
      "         [[-0.0413, -0.0226, -0.1341],\n",
      "          [ 0.0077,  0.0239,  0.1158],\n",
      "          [-0.0434, -0.1356, -0.0957]],\n",
      "\n",
      "         [[-0.0938, -0.0840, -0.1258],\n",
      "          [ 0.0594, -0.0938, -0.0047],\n",
      "          [-0.0532,  0.1115, -0.0518]],\n",
      "\n",
      "         [[-0.0862, -0.0041,  0.0204],\n",
      "          [ 0.1066, -0.1278,  0.0943],\n",
      "          [-0.0076, -0.0053, -0.0942]],\n",
      "\n",
      "         [[-0.0416,  0.0154, -0.0383],\n",
      "          [ 0.0803,  0.0787,  0.0445],\n",
      "          [ 0.0373, -0.0621,  0.0212]],\n",
      "\n",
      "         [[ 0.0102,  0.1328, -0.0424],\n",
      "          [-0.1148, -0.1225, -0.1168],\n",
      "          [-0.0047,  0.1328,  0.0077]]],\n",
      "\n",
      "\n",
      "        [[[-0.0581, -0.0419, -0.0752],\n",
      "          [-0.0070, -0.0727, -0.1353],\n",
      "          [-0.0214, -0.0517, -0.0475]],\n",
      "\n",
      "         [[-0.0036, -0.1260,  0.1013],\n",
      "          [-0.1332, -0.0132,  0.0109],\n",
      "          [ 0.0128,  0.0810,  0.1073]],\n",
      "\n",
      "         [[ 0.0922, -0.0105,  0.0394],\n",
      "          [-0.0190, -0.1319,  0.0896],\n",
      "          [ 0.1280, -0.0412, -0.0674]],\n",
      "\n",
      "         [[ 0.1117,  0.1174, -0.0129],\n",
      "          [-0.0900, -0.0199, -0.0190],\n",
      "          [ 0.0486, -0.0743,  0.0736]],\n",
      "\n",
      "         [[-0.0488,  0.1292,  0.0531],\n",
      "          [ 0.1172,  0.0382,  0.0182],\n",
      "          [-0.0273,  0.1053, -0.0983]],\n",
      "\n",
      "         [[ 0.0362, -0.0200,  0.0868],\n",
      "          [ 0.0327, -0.0954, -0.1060],\n",
      "          [-0.0750,  0.0879,  0.0503]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1262, -0.1350, -0.0110],\n",
      "          [ 0.0921,  0.0978, -0.1032],\n",
      "          [-0.1352, -0.0384, -0.1130]],\n",
      "\n",
      "         [[-0.0683, -0.1055,  0.0615],\n",
      "          [ 0.0459,  0.0753,  0.0201],\n",
      "          [ 0.0781, -0.0783, -0.0522]],\n",
      "\n",
      "         [[-0.0263, -0.1052, -0.1223],\n",
      "          [ 0.0118, -0.0549,  0.0121],\n",
      "          [ 0.0673, -0.0403,  0.0384]],\n",
      "\n",
      "         [[ 0.0122,  0.0175,  0.0276],\n",
      "          [ 0.0082,  0.0435, -0.0786],\n",
      "          [-0.1304,  0.0921,  0.1354]],\n",
      "\n",
      "         [[ 0.0053, -0.0301,  0.0168],\n",
      "          [ 0.0022,  0.1120,  0.0476],\n",
      "          [-0.0415, -0.0239, -0.1110]],\n",
      "\n",
      "         [[-0.1282, -0.1141,  0.0043],\n",
      "          [ 0.0602,  0.0896, -0.0814],\n",
      "          [-0.0468, -0.0929, -0.1214]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0346, -0.0858, -0.1177],\n",
      "          [ 0.1026,  0.0572, -0.0038],\n",
      "          [-0.0456,  0.1284, -0.1005]],\n",
      "\n",
      "         [[ 0.0651, -0.0945, -0.1105],\n",
      "          [-0.0829, -0.0266, -0.1130],\n",
      "          [-0.0472, -0.0976, -0.1156]],\n",
      "\n",
      "         [[-0.0764, -0.1182, -0.0374],\n",
      "          [-0.0004, -0.0313,  0.0068],\n",
      "          [ 0.0137,  0.0980,  0.0387]],\n",
      "\n",
      "         [[-0.0537, -0.0699, -0.0111],\n",
      "          [ 0.0879,  0.0395,  0.0594],\n",
      "          [ 0.0114, -0.1256,  0.0057]],\n",
      "\n",
      "         [[-0.0450,  0.0924, -0.0601],\n",
      "          [-0.0021, -0.1266, -0.0817],\n",
      "          [-0.0838,  0.1244, -0.0157]],\n",
      "\n",
      "         [[-0.0319, -0.0707,  0.1215],\n",
      "          [-0.1110, -0.0527, -0.1161],\n",
      "          [-0.0995,  0.0016,  0.0056]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0242, -0.1115, -0.0711],\n",
      "          [-0.0486,  0.1057,  0.0773],\n",
      "          [ 0.1321, -0.0613, -0.0160]],\n",
      "\n",
      "         [[ 0.0532, -0.0508,  0.0689],\n",
      "          [ 0.0560,  0.0213, -0.0518],\n",
      "          [ 0.0218,  0.1183, -0.0347]],\n",
      "\n",
      "         [[-0.0213, -0.0808, -0.1083],\n",
      "          [-0.0801, -0.0114,  0.0797],\n",
      "          [-0.1113,  0.0884,  0.1126]],\n",
      "\n",
      "         [[ 0.0858,  0.0590,  0.0176],\n",
      "          [ 0.0109, -0.0186,  0.0855],\n",
      "          [ 0.0766,  0.0663, -0.0809]],\n",
      "\n",
      "         [[ 0.0469,  0.1024,  0.1325],\n",
      "          [ 0.1237, -0.0610,  0.0842],\n",
      "          [-0.1349,  0.1209,  0.0962]],\n",
      "\n",
      "         [[ 0.0602,  0.0333,  0.0038],\n",
      "          [-0.1035,  0.0502, -0.1289],\n",
      "          [ 0.0424, -0.0241, -0.0560]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0739,  0.0992,  0.0227],\n",
      "          [ 0.0580, -0.1342,  0.0325],\n",
      "          [ 0.0941,  0.0966, -0.0802]],\n",
      "\n",
      "         [[-0.0495, -0.1202, -0.0075],\n",
      "          [-0.0959, -0.0238,  0.0698],\n",
      "          [-0.0084,  0.0354,  0.0448]],\n",
      "\n",
      "         [[-0.0603,  0.0702,  0.0029],\n",
      "          [-0.0245, -0.1139,  0.1222],\n",
      "          [-0.0621, -0.0430, -0.0704]],\n",
      "\n",
      "         [[-0.0617,  0.0314,  0.0040],\n",
      "          [ 0.0515,  0.1345, -0.0851],\n",
      "          [ 0.0924,  0.0894,  0.0534]],\n",
      "\n",
      "         [[ 0.0802,  0.1296, -0.0091],\n",
      "          [ 0.0982,  0.0904, -0.1052],\n",
      "          [-0.0490, -0.0504,  0.0317]],\n",
      "\n",
      "         [[-0.0893,  0.0018,  0.1281],\n",
      "          [ 0.1188,  0.1130,  0.0469],\n",
      "          [ 0.0298, -0.0259,  0.0055]]],\n",
      "\n",
      "\n",
      "        [[[-0.0564,  0.1048, -0.1347],\n",
      "          [ 0.0726,  0.1243, -0.0097],\n",
      "          [-0.0201,  0.0350,  0.0765]],\n",
      "\n",
      "         [[-0.1180, -0.0448, -0.0547],\n",
      "          [-0.0218,  0.0272,  0.0775],\n",
      "          [-0.0622, -0.0484, -0.1119]],\n",
      "\n",
      "         [[ 0.0583,  0.0384, -0.0042],\n",
      "          [ 0.0075,  0.0753,  0.0063],\n",
      "          [-0.0321,  0.1179,  0.1211]],\n",
      "\n",
      "         [[ 0.0299,  0.0903, -0.0517],\n",
      "          [ 0.0441, -0.0640,  0.1039],\n",
      "          [-0.0930, -0.1275,  0.0834]],\n",
      "\n",
      "         [[ 0.0653, -0.0221, -0.0754],\n",
      "          [ 0.1296,  0.0311,  0.0521],\n",
      "          [-0.0284,  0.0966,  0.0438]],\n",
      "\n",
      "         [[-0.0742, -0.1095, -0.0192],\n",
      "          [-0.0853, -0.0496, -0.0577],\n",
      "          [ 0.0222,  0.0864, -0.0897]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0034, -0.1175,  0.0793],\n",
      "          [-0.0112,  0.0453,  0.1044],\n",
      "          [ 0.0446,  0.0003, -0.0754]],\n",
      "\n",
      "         [[ 0.0080, -0.0313, -0.1233],\n",
      "          [-0.0199, -0.0966,  0.0965],\n",
      "          [-0.0163, -0.1308,  0.0967]],\n",
      "\n",
      "         [[-0.1111, -0.0268, -0.0514],\n",
      "          [-0.1161,  0.0622,  0.1167],\n",
      "          [ 0.1024,  0.0139, -0.0797]],\n",
      "\n",
      "         [[ 0.0169,  0.1269, -0.0699],\n",
      "          [-0.1015, -0.0188,  0.0607],\n",
      "          [-0.0048, -0.0229,  0.0171]],\n",
      "\n",
      "         [[-0.1202,  0.1310,  0.0766],\n",
      "          [-0.0807,  0.0278, -0.0825],\n",
      "          [-0.0860, -0.1201, -0.0207]],\n",
      "\n",
      "         [[-0.0796, -0.0434, -0.0809],\n",
      "          [ 0.0720,  0.1181,  0.0062],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          [ 0.1043, -0.1267,  0.1022]]]], requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([[[[-0.0551, -0.0107, -0.0201],\n",
      "          [ 0.1325, -0.0647, -0.0104],\n",
      "          [-0.0324,  0.0699,  0.0174]],\n",
      "\n",
      "         [[ 0.0037,  0.0720, -0.0699],\n",
      "          [-0.0332,  0.0648, -0.0979],\n",
      "          [ 0.1019, -0.0140,  0.0129]],\n",
      "\n",
      "         [[ 0.0032, -0.0138, -0.0610],\n",
      "          [ 0.0222, -0.1320, -0.0412],\n",
      "          [-0.1174, -0.0175, -0.0659]],\n",
      "\n",
      "         [[ 0.0744,  0.0351,  0.0096],\n",
      "          [-0.1138,  0.0353,  0.0931],\n",
      "          [ 0.0489, -0.0581, -0.0654]],\n",
      "\n",
      "         [[-0.1357,  0.0610,  0.0415],\n",
      "          [ 0.0988, -0.0274, -0.0571],\n",
      "          [ 0.0121,  0.0051, -0.0400]],\n",
      "\n",
      "         [[-0.1074, -0.0150,  0.0380],\n",
      "          [-0.0212, -0.1051,  0.1030],\n",
      "          [ 0.0268, -0.0362,  0.0681]]],\n",
      "\n",
      "\n",
      "        [[[-0.0788, -0.0654,  0.1232],\n",
      "          [ 0.1329, -0.0598,  0.0565],\n",
      "          [-0.1005, -0.0167, -0.0329]],\n",
      "\n",
      "         [[-0.0677, -0.1262,  0.0469],\n",
      "          [-0.0848, -0.0422, -0.0672],\n",
      "          [-0.0057, -0.0232,  0.0045]],\n",
      "\n",
      "         [[ 0.0620,  0.1002, -0.0686],\n",
      "          [ 0.0981, -0.0550, -0.0006],\n",
      "          [ 0.0191, -0.0286, -0.0179]],\n",
      "\n",
      "         [[ 0.0589, -0.1321, -0.1287],\n",
      "          [-0.0579,  0.0748,  0.0379],\n",
      "          [ 0.0213, -0.0323,  0.0987]],\n",
      "\n",
      "         [[ 0.1173,  0.0853, -0.0473],\n",
      "          [-0.1146,  0.0352, -0.1034],\n",
      "          [-0.0410,  0.1021, -0.0275]],\n",
      "\n",
      "         [[ 0.1006,  0.0712, -0.0453],\n",
      "          [ 0.1285,  0.1157,  0.1226],\n",
      "          [-0.0566, -0.0930, -0.1045]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0289,  0.1105, -0.1285],\n",
      "          [ 0.1082, -0.0223,  0.0795],\n",
      "          [ 0.0417,  0.0811,  0.0768]],\n",
      "\n",
      "         [[-0.0649, -0.0062, -0.0866],\n",
      "          [-0.0603, -0.0931,  0.0250],\n",
      "          [ 0.0687,  0.1185, -0.1080]],\n",
      "\n",
      "         [[-0.0413,  0.0590,  0.1146],\n",
      "          [ 0.0831, -0.0652, -0.0114],\n",
      "          [ 0.0681, -0.0463, -0.1046]],\n",
      "\n",
      "         [[ 0.0312,  0.1142,  0.1209],\n",
      "          [ 0.1290,  0.0272, -0.0976],\n",
      "          [-0.0491,  0.1131,  0.1090]],\n",
      "\n",
      "         [[ 0.0305,  0.1138,  0.0952],\n",
      "          [ 0.0969,  0.1345, -0.0685],\n",
      "          [ 0.0876, -0.0209,  0.0503]],\n",
      "\n",
      "         [[ 0.0126,  0.0963,  0.0884],\n",
      "          [-0.0552,  0.0654,  0.1224],\n",
      "          [-0.0163,  0.0039,  0.0639]]],\n",
      "\n",
      "\n",
      "        [[[-0.0427, -0.1051,  0.0858],\n",
      "          [-0.0357, -0.0559,  0.0471],\n",
      "          [ 0.0770, -0.0486, -0.1151]],\n",
      "\n",
      "         [[ 0.0758,  0.0194,  0.0957],\n",
      "          [-0.0278, -0.0462, -0.1244],\n",
      "          [-0.0754, -0.0969,  0.0037]],\n",
      "\n",
      "         [[ 0.0857,  0.0358, -0.0868],\n",
      "          [ 0.0366,  0.0235, -0.0208],\n",
      "          [-0.0187,  0.1345,  0.0381]],\n",
      "\n",
      "         [[ 0.1214,  0.0017, -0.0346],\n",
      "          [-0.1062,  0.0052,  0.0548],\n",
      "          [ 0.0330, -0.0294,  0.0708]],\n",
      "\n",
      "         [[ 0.1171,  0.0164,  0.0566],\n",
      "          [ 0.1013, -0.0071,  0.0359],\n",
      "          [ 0.0433,  0.0820,  0.1329]],\n",
      "\n",
      "         [[-0.0879,  0.0342, -0.0311],\n",
      "          [ 0.0950,  0.1101,  0.0558],\n",
      "          [ 0.0494,  0.0357,  0.0443]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0702,  0.0065,  0.0368],\n",
      "          [ 0.1150,  0.0528, -0.0672],\n",
      "          [ 0.0470, -0.1125,  0.0719]],\n",
      "\n",
      "         [[ 0.1033, -0.0245,  0.0256],\n",
      "          [ 0.0715, -0.0231,  0.0858],\n",
      "          [-0.1287, -0.0992, -0.0497]],\n",
      "\n",
      "         [[ 0.1287, -0.0247,  0.0189],\n",
      "          [ 0.0298, -0.0615, -0.0939],\n",
      "          [-0.0691, -0.0513, -0.0509]],\n",
      "\n",
      "         [[-0.0446, -0.0517,  0.0291],\n",
      "          [ 0.0539, -0.0253, -0.1129],\n",
      "          [ 0.0524,  0.0170,  0.1148]],\n",
      "\n",
      "         [[-0.1018,  0.0161,  0.1312],\n",
      "          [-0.0481,  0.0105,  0.0734],\n",
      "          [-0.1033, -0.0089,  0.0386]],\n",
      "\n",
      "         [[ 0.0217, -0.1003, -0.0265],\n",
      "          [-0.0076,  0.0109,  0.0556],\n",
      "          [-0.0136, -0.1224,  0.0525]]],\n",
      "\n",
      "\n",
      "        [[[-0.0864, -0.0424, -0.0411],\n",
      "          [-0.1162,  0.1216,  0.0150],\n",
      "          [ 0.0742, -0.1047, -0.1014]],\n",
      "\n",
      "         [[-0.0415,  0.0566,  0.0578],\n",
      "          [ 0.0749, -0.0073, -0.1082],\n",
      "          [ 0.0252, -0.0517, -0.0993]],\n",
      "\n",
      "         [[ 0.0296,  0.0098, -0.1132],\n",
      "          [-0.1076, -0.1024,  0.1245],\n",
      "          [ 0.1023,  0.0576, -0.0691]],\n",
      "\n",
      "         [[ 0.0557, -0.0107,  0.0583],\n",
      "          [-0.1168, -0.0673, -0.1309],\n",
      "          [-0.1083,  0.0041, -0.0282]],\n",
      "\n",
      "         [[ 0.0580,  0.1331,  0.0963],\n",
      "          [ 0.1249, -0.0683,  0.0148],\n",
      "          [-0.0625,  0.0251,  0.0501]],\n",
      "\n",
      "         [[ 0.0620,  0.0237,  0.0761],\n",
      "          [ 0.1242, -0.0485, -0.0369],\n",
      "          [-0.0415, -0.0958, -0.0814]]],\n",
      "\n",
      "\n",
      "        [[[-0.0968, -0.0259,  0.0349],\n",
      "          [-0.0731, -0.0851,  0.0677],\n",
      "          [-0.0227, -0.0317,  0.1169]],\n",
      "\n",
      "         [[ 0.0644, -0.0867,  0.0654],\n",
      "          [ 0.0628,  0.0912, -0.0285],\n",
      "          [ 0.0878,  0.0596,  0.0030]],\n",
      "\n",
      "         [[-0.0802,  0.1273, -0.0118],\n",
      "          [-0.0910, -0.0834, -0.0512],\n",
      "          [-0.0814, -0.0588,  0.1177]],\n",
      "\n",
      "         [[-0.1282,  0.1094, -0.0228],\n",
      "          [ 0.0073,  0.0087, -0.0476],\n",
      "          [ 0.1111, -0.0858, -0.0837]],\n",
      "\n",
      "         [[ 0.1358, -0.1054, -0.0156],\n",
      "          [-0.0783, -0.0707, -0.1145],\n",
      "          [ 0.0242, -0.0161, -0.0846]],\n",
      "\n",
      "         [[ 0.0056, -0.0160,  0.0331],\n",
      "          [-0.0343, -0.1351, -0.1078],\n",
      "          [ 0.1037,  0.0162,  0.0300]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0780,  0.1295, -0.1144],\n",
      "          [-0.1046,  0.0592, -0.0454],\n",
      "          [-0.1283,  0.0118,  0.0063]],\n",
      "\n",
      "         [[-0.0916, -0.0069,  0.0149],\n",
      "          [ 0.1210, -0.0818, -0.0530],\n",
      "          [ 0.0102,  0.0466,  0.0501]],\n",
      "\n",
      "         [[-0.0884, -0.0155,  0.0303],\n",
      "          [-0.0539,  0.0183,  0.0353],\n",
      "          [ 0.1216,  0.0198, -0.0123]],\n",
      "\n",
      "         [[ 0.1115,  0.0088,  0.0521],\n",
      "          [-0.0012,  0.0200, -0.1087],\n",
      "          [-0.0125, -0.1292, -0.0124]],\n",
      "\n",
      "         [[ 0.1259,  0.1254, -0.0920],\n",
      "          [ 0.1332,  0.1155,  0.0377],\n",
      "          [-0.0247,  0.0593,  0.1200]],\n",
      "\n",
      "         [[ 0.1055,  0.0821,  0.0332],\n",
      "          [ 0.0432, -0.0456, -0.0008],\n",
      "          [-0.1323,  0.1303,  0.0225]]],\n",
      "\n",
      "\n",
      "        [[[-0.1006,  0.0800,  0.0117],\n",
      "          [ 0.0029,  0.0823, -0.0203],\n",
      "          [ 0.0955, -0.0870, -0.0383]],\n",
      "\n",
      "         [[-0.0413, -0.0226, -0.1341],\n",
      "          [ 0.0077,  0.0239,  0.1158],\n",
      "          [-0.0434, -0.1356, -0.0957]],\n",
      "\n",
      "         [[-0.0938, -0.0839, -0.1258],\n",
      "          [ 0.0594, -0.0938, -0.0047],\n",
      "          [-0.0532,  0.1115, -0.0518]],\n",
      "\n",
      "         [[-0.0861, -0.0041,  0.0204],\n",
      "          [ 0.1066, -0.1278,  0.0943],\n",
      "          [-0.0076, -0.0053, -0.0942]],\n",
      "\n",
      "         [[-0.0416,  0.0153, -0.0383],\n",
      "          [ 0.0803,  0.0787,  0.0445],\n",
      "          [ 0.0372, -0.0621,  0.0212]],\n",
      "\n",
      "         [[ 0.0102,  0.1328, -0.0424],\n",
      "          [-0.1148, -0.1225, -0.1168],\n",
      "          [-0.0047,  0.1328,  0.0077]]],\n",
      "\n",
      "\n",
      "        [[[-0.0581, -0.0419, -0.0752],\n",
      "          [-0.0069, -0.0727, -0.1352],\n",
      "          [-0.0214, -0.0517, -0.0474]],\n",
      "\n",
      "         [[-0.0036, -0.1260,  0.1013],\n",
      "          [-0.1332, -0.0132,  0.0110],\n",
      "          [ 0.0128,  0.0810,  0.1073]],\n",
      "\n",
      "         [[ 0.0922, -0.0104,  0.0394],\n",
      "          [-0.0190, -0.1319,  0.0897],\n",
      "          [ 0.1280, -0.0411, -0.0674]],\n",
      "\n",
      "         [[ 0.1116,  0.1174, -0.0129],\n",
      "          [-0.0900, -0.0198, -0.0189],\n",
      "          [ 0.0486, -0.0743,  0.0737]],\n",
      "\n",
      "         [[-0.0489,  0.1293,  0.0531],\n",
      "          [ 0.1172,  0.0382,  0.0182],\n",
      "          [-0.0273,  0.1053, -0.0983]],\n",
      "\n",
      "         [[ 0.0363, -0.0200,  0.0868],\n",
      "          [ 0.0326, -0.0954, -0.1060],\n",
      "          [-0.0750,  0.0879,  0.0503]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1262, -0.1350, -0.0110],\n",
      "          [ 0.0921,  0.0978, -0.1032],\n",
      "          [-0.1352, -0.0384, -0.1130]],\n",
      "\n",
      "         [[-0.0683, -0.1055,  0.0615],\n",
      "          [ 0.0459,  0.0753,  0.0201],\n",
      "          [ 0.0781, -0.0783, -0.0522]],\n",
      "\n",
      "         [[-0.0263, -0.1052, -0.1223],\n",
      "          [ 0.0118, -0.0549,  0.0121],\n",
      "          [ 0.0673, -0.0403,  0.0384]],\n",
      "\n",
      "         [[ 0.0122,  0.0175,  0.0276],\n",
      "          [ 0.0082,  0.0435, -0.0786],\n",
      "          [-0.1304,  0.0921,  0.1354]],\n",
      "\n",
      "         [[ 0.0053, -0.0301,  0.0168],\n",
      "          [ 0.0022,  0.1120,  0.0476],\n",
      "          [-0.0415, -0.0239, -0.1110]],\n",
      "\n",
      "         [[-0.1282, -0.1141,  0.0043],\n",
      "          [ 0.0602,  0.0896, -0.0814],\n",
      "          [-0.0468, -0.0929, -0.1214]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0346, -0.0858, -0.1177],\n",
      "          [ 0.1026,  0.0572, -0.0038],\n",
      "          [-0.0456,  0.1284, -0.1005]],\n",
      "\n",
      "         [[ 0.0651, -0.0945, -0.1105],\n",
      "          [-0.0829, -0.0266, -0.1130],\n",
      "          [-0.0472, -0.0976, -0.1156]],\n",
      "\n",
      "         [[-0.0764, -0.1182, -0.0374],\n",
      "          [-0.0004, -0.0313,  0.0068],\n",
      "          [ 0.0137,  0.0980,  0.0387]],\n",
      "\n",
      "         [[-0.0537, -0.0699, -0.0111],\n",
      "          [ 0.0879,  0.0395,  0.0594],\n",
      "          [ 0.0114, -0.1256,  0.0057]],\n",
      "\n",
      "         [[-0.0450,  0.0924, -0.0601],\n",
      "          [-0.0021, -0.1266, -0.0817],\n",
      "          [-0.0838,  0.1244, -0.0157]],\n",
      "\n",
      "         [[-0.0319, -0.0707,  0.1215],\n",
      "          [-0.1110, -0.0527, -0.1161],\n",
      "          [-0.0995,  0.0016,  0.0056]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0243, -0.1115, -0.0710],\n",
      "          [-0.0486,  0.1057,  0.0773],\n",
      "          [ 0.1321, -0.0613, -0.0160]],\n",
      "\n",
      "         [[ 0.0533, -0.0507,  0.0690],\n",
      "          [ 0.0560,  0.0214, -0.0518],\n",
      "          [ 0.0219,  0.1184, -0.0346]],\n",
      "\n",
      "         [[-0.0212, -0.0807, -0.1082],\n",
      "          [-0.0801, -0.0113,  0.0797],\n",
      "          [-0.1113,  0.0884,  0.1126]],\n",
      "\n",
      "         [[ 0.0858,  0.0590,  0.0176],\n",
      "          [ 0.0110, -0.0186,  0.0856],\n",
      "          [ 0.0767,  0.0664, -0.0808]],\n",
      "\n",
      "         [[ 0.0469,  0.1024,  0.1326],\n",
      "          [ 0.1238, -0.0609,  0.0842],\n",
      "          [-0.1348,  0.1210,  0.0963]],\n",
      "\n",
      "         [[ 0.0603,  0.0334,  0.0038],\n",
      "          [-0.1035,  0.0502, -0.1289],\n",
      "          [ 0.0425, -0.0240, -0.0560]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0739,  0.0992,  0.0227],\n",
      "          [ 0.0580, -0.1342,  0.0326],\n",
      "          [ 0.0941,  0.0966, -0.0802]],\n",
      "\n",
      "         [[-0.0495, -0.1201, -0.0075],\n",
      "          [-0.0959, -0.0238,  0.0699],\n",
      "          [-0.0085,  0.0355,  0.0447]],\n",
      "\n",
      "         [[-0.0603,  0.0702,  0.0030],\n",
      "          [-0.0245, -0.1139,  0.1222],\n",
      "          [-0.0621, -0.0431, -0.0704]],\n",
      "\n",
      "         [[-0.0617,  0.0314,  0.0041],\n",
      "          [ 0.0514,  0.1345, -0.0850],\n",
      "          [ 0.0924,  0.0895,  0.0534]],\n",
      "\n",
      "         [[ 0.0802,  0.1296, -0.0091],\n",
      "          [ 0.0982,  0.0904, -0.1051],\n",
      "          [-0.0490, -0.0504,  0.0318]],\n",
      "\n",
      "         [[-0.0892,  0.0017,  0.1281],\n",
      "          [ 0.1189,  0.1130,  0.0469],\n",
      "          [ 0.0298, -0.0259,  0.0055]]],\n",
      "\n",
      "\n",
      "        [[[-0.0564,  0.1048, -0.1347],\n",
      "          [ 0.0726,  0.1244, -0.0097],\n",
      "          [-0.0201,  0.0351,  0.0765]],\n",
      "\n",
      "         [[-0.1180, -0.0448, -0.0546],\n",
      "          [-0.0218,  0.0272,  0.0775],\n",
      "          [-0.0622, -0.0483, -0.1118]],\n",
      "\n",
      "         [[ 0.0584,  0.0384, -0.0042],\n",
      "          [ 0.0075,  0.0752,  0.0063],\n",
      "          [-0.0321,  0.1180,  0.1211]],\n",
      "\n",
      "         [[ 0.0299,  0.0904, -0.0517],\n",
      "          [ 0.0441, -0.0640,  0.1038],\n",
      "          [-0.0930, -0.1274,  0.0834]],\n",
      "\n",
      "         [[ 0.0653, -0.0222, -0.0754],\n",
      "          [ 0.1296,  0.0311,  0.0521],\n",
      "          [-0.0285,  0.0966,  0.0438]],\n",
      "\n",
      "         [[-0.0742, -0.1095, -0.0192],\n",
      "          [-0.0853, -0.0495, -0.0577],\n",
      "          [ 0.0222,  0.0864, -0.0897]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0034, -0.1175,  0.0793],\n",
      "          [-0.0112,  0.0453,  0.1044],\n",
      "          [ 0.0446,  0.0003, -0.0754]],\n",
      "\n",
      "         [[ 0.0080, -0.0313, -0.1233],\n",
      "          [-0.0199, -0.0965,  0.0965],\n",
      "          [-0.0163, -0.1308,  0.0967]],\n",
      "\n",
      "         [[-0.1110, -0.0268, -0.0514],\n",
      "          [-0.1161,  0.0622,  0.1168],\n",
      "          [ 0.1023,  0.0139, -0.0797]],\n",
      "\n",
      "         [[ 0.0169,  0.1269, -0.0699],\n",
      "          [-0.1015, -0.0188,  0.0608],\n",
      "          [-0.0048, -0.0229,  0.0171]],\n",
      "\n",
      "         [[-0.1202,  0.1311,  0.0766],\n",
      "          [-0.0808,  0.0278, -0.0825],\n",
      "          [-0.0860, -0.1201, -0.0207]],\n",
      "\n",
      "         [[-0.0796, -0.0434, -0.0809],\n",
      "          [ 0.0720,  0.1181,  0.0062],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          [ 0.1043, -0.1267,  0.1022]]]], requires_grad=True)\n",
      "before Parameter containing:\n",
      "tensor([-0.1277, -0.0906,  0.0798,  0.0852, -0.0608, -0.0266, -0.0492, -0.0152,\n",
      "         0.0487, -0.1313, -0.0628,  0.0072, -0.0134,  0.0911,  0.1181,  0.0550],\n",
      "       requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([-0.1277, -0.0906,  0.0798,  0.0852, -0.0608, -0.0266, -0.0492, -0.0153,\n",
      "         0.0487, -0.1313, -0.0628,  0.0072, -0.0133,  0.0911,  0.1181,  0.0550],\n",
      "       requires_grad=True)\n",
      "before Parameter containing:\n",
      "tensor([[-0.0242, -0.0018, -0.0076,  ...,  0.0186,  0.0158, -0.0014],\n",
      "        [ 0.0272,  0.0129, -0.0289,  ..., -0.0406,  0.0299,  0.0018],\n",
      "        [ 0.0031,  0.0121, -0.0375,  ...,  0.0046,  0.0412, -0.0196],\n",
      "        ...,\n",
      "        [-0.0252, -0.0026,  0.0082,  ...,  0.0354, -0.0300, -0.0178],\n",
      "        [ 0.0152, -0.0228,  0.0307,  ..., -0.0297,  0.0330, -0.0232],\n",
      "        [ 0.0006, -0.0185, -0.0367,  ...,  0.0009, -0.0176, -0.0022]],\n",
      "       requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([[-0.0242, -0.0018, -0.0076,  ...,  0.0186,  0.0158, -0.0014],\n",
      "        [ 0.0272,  0.0129, -0.0289,  ..., -0.0406,  0.0299,  0.0018],\n",
      "        [ 0.0031,  0.0121, -0.0375,  ...,  0.0046,  0.0412, -0.0196],\n",
      "        ...,\n",
      "        [-0.0252, -0.0026,  0.0082,  ...,  0.0354, -0.0300, -0.0178],\n",
      "        [ 0.0152, -0.0228,  0.0307,  ..., -0.0297,  0.0330, -0.0232],\n",
      "        [ 0.0006, -0.0185, -0.0367,  ...,  0.0009, -0.0176, -0.0022]],\n",
      "       requires_grad=True)\n",
      "before Parameter containing:\n",
      "tensor([ 0.0417, -0.0140,  0.0005, -0.0251,  0.0064, -0.0023,  0.0333, -0.0413,\n",
      "         0.0292,  0.0214,  0.0345, -0.0091, -0.0010, -0.0047,  0.0174,  0.0288,\n",
      "         0.0004, -0.0144, -0.0227,  0.0207, -0.0299,  0.0097,  0.0265, -0.0251,\n",
      "        -0.0065, -0.0096, -0.0322,  0.0043,  0.0380,  0.0398, -0.0135, -0.0241,\n",
      "        -0.0154,  0.0198,  0.0057,  0.0376,  0.0169, -0.0373, -0.0023,  0.0234,\n",
      "         0.0015, -0.0134,  0.0136,  0.0066,  0.0361, -0.0057, -0.0229,  0.0197,\n",
      "         0.0151,  0.0387,  0.0133, -0.0199,  0.0241, -0.0318, -0.0110,  0.0033,\n",
      "         0.0328,  0.0187, -0.0175,  0.0201,  0.0147, -0.0250,  0.0405,  0.0013,\n",
      "        -0.0384, -0.0258,  0.0405, -0.0348, -0.0037,  0.0266, -0.0080, -0.0244,\n",
      "         0.0230, -0.0043, -0.0120, -0.0330,  0.0275, -0.0098, -0.0125, -0.0055,\n",
      "         0.0047, -0.0007, -0.0293,  0.0243,  0.0283, -0.0120,  0.0161, -0.0027,\n",
      "        -0.0351, -0.0004,  0.0287, -0.0114,  0.0296, -0.0075, -0.0300,  0.0132,\n",
      "         0.0201, -0.0135, -0.0238, -0.0123, -0.0009, -0.0184,  0.0019,  0.0235,\n",
      "         0.0393, -0.0009, -0.0203,  0.0306,  0.0362,  0.0237,  0.0183, -0.0037,\n",
      "        -0.0157,  0.0340,  0.0379,  0.0379,  0.0393, -0.0010, -0.0042, -0.0167],\n",
      "       requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([ 0.0417, -0.0140,  0.0005, -0.0251,  0.0064, -0.0023,  0.0332, -0.0413,\n",
      "         0.0293,  0.0214,  0.0346, -0.0091, -0.0010, -0.0047,  0.0175,  0.0288,\n",
      "         0.0004, -0.0144, -0.0227,  0.0207, -0.0300,  0.0097,  0.0264, -0.0251,\n",
      "        -0.0065, -0.0096, -0.0322,  0.0043,  0.0380,  0.0398, -0.0135, -0.0242,\n",
      "        -0.0153,  0.0198,  0.0055,  0.0376,  0.0168, -0.0374, -0.0023,  0.0234,\n",
      "         0.0016, -0.0134,  0.0136,  0.0066,  0.0360, -0.0057, -0.0229,  0.0197,\n",
      "         0.0151,  0.0387,  0.0134, -0.0199,  0.0241, -0.0318, -0.0110,  0.0033,\n",
      "         0.0328,  0.0187, -0.0176,  0.0201,  0.0147, -0.0250,  0.0405,  0.0013,\n",
      "        -0.0385, -0.0258,  0.0405, -0.0348, -0.0037,  0.0266, -0.0080, -0.0244,\n",
      "         0.0230, -0.0043, -0.0120, -0.0329,  0.0275, -0.0098, -0.0125, -0.0055,\n",
      "         0.0047, -0.0009, -0.0292,  0.0244,  0.0283, -0.0119,  0.0162, -0.0027,\n",
      "        -0.0350, -0.0004,  0.0287, -0.0114,  0.0296, -0.0075, -0.0300,  0.0133,\n",
      "         0.0200, -0.0136, -0.0238, -0.0122, -0.0010, -0.0184,  0.0019,  0.0235,\n",
      "         0.0394, -0.0009, -0.0203,  0.0305,  0.0363,  0.0237,  0.0183, -0.0037,\n",
      "        -0.0157,  0.0340,  0.0378,  0.0380,  0.0393, -0.0010, -0.0042, -0.0167],\n",
      "       requires_grad=True)\n",
      "before Parameter containing:\n",
      "tensor([[-0.0431,  0.0642, -0.0228,  ...,  0.0115, -0.0040, -0.0893],\n",
      "        [ 0.0477, -0.0304,  0.0813,  ...,  0.0702, -0.0558,  0.0247],\n",
      "        [-0.0613,  0.0869, -0.0839,  ..., -0.0152,  0.0341,  0.0649],\n",
      "        ...,\n",
      "        [-0.0765, -0.0609, -0.0263,  ..., -0.0760,  0.0023,  0.0741],\n",
      "        [-0.0484, -0.0616,  0.0041,  ...,  0.0874,  0.0633, -0.0840],\n",
      "        [ 0.0794, -0.0904, -0.0823,  ...,  0.0657,  0.0343, -0.0438]],\n",
      "       requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([[-0.0431,  0.0642, -0.0228,  ...,  0.0115, -0.0040, -0.0893],\n",
      "        [ 0.0477, -0.0304,  0.0813,  ...,  0.0702, -0.0558,  0.0247],\n",
      "        [-0.0613,  0.0869, -0.0839,  ..., -0.0152,  0.0341,  0.0649],\n",
      "        ...,\n",
      "        [-0.0765, -0.0609, -0.0263,  ..., -0.0760,  0.0023,  0.0741],\n",
      "        [-0.0484, -0.0616,  0.0041,  ...,  0.0874,  0.0633, -0.0840],\n",
      "        [ 0.0794, -0.0904, -0.0823,  ...,  0.0657,  0.0343, -0.0438]],\n",
      "       requires_grad=True)\n",
      "before Parameter containing:\n",
      "tensor([-0.0671,  0.0883, -0.0467, -0.0470,  0.0104,  0.0893,  0.0287, -0.0903,\n",
      "        -0.0087, -0.0735,  0.0499, -0.0290, -0.0670, -0.0598,  0.0730,  0.0787,\n",
      "         0.0506, -0.0435,  0.0590,  0.0674, -0.0308,  0.0710,  0.0085, -0.0844,\n",
      "        -0.0406, -0.0850,  0.0158, -0.0362, -0.0561,  0.0372,  0.0898, -0.0465,\n",
      "         0.0409,  0.0027,  0.0371,  0.0194, -0.0374,  0.0770, -0.0251, -0.0074,\n",
      "         0.0094, -0.0610, -0.0355,  0.0585, -0.0155,  0.0693, -0.0634,  0.0654,\n",
      "        -0.0075, -0.0618,  0.0430, -0.0529, -0.0532,  0.0225,  0.0866,  0.0246,\n",
      "         0.0697,  0.0546,  0.0016, -0.0596,  0.0703, -0.0504, -0.0542,  0.0060,\n",
      "        -0.0900, -0.0422, -0.0294,  0.0302, -0.0377,  0.0482,  0.0212, -0.0523,\n",
      "         0.0753, -0.0672,  0.0052,  0.0225,  0.0526, -0.0897, -0.0038,  0.0163,\n",
      "        -0.0303, -0.0873, -0.0179, -0.0721], requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([-0.0671,  0.0883, -0.0467, -0.0470,  0.0105,  0.0893,  0.0287, -0.0903,\n",
      "        -0.0084, -0.0735,  0.0497, -0.0290, -0.0670, -0.0598,  0.0730,  0.0787,\n",
      "         0.0506, -0.0435,  0.0591,  0.0674, -0.0308,  0.0711,  0.0085, -0.0844,\n",
      "        -0.0406, -0.0853,  0.0155, -0.0362, -0.0561,  0.0374,  0.0898, -0.0470,\n",
      "         0.0407,  0.0029,  0.0371,  0.0193, -0.0375,  0.0770, -0.0251, -0.0074,\n",
      "         0.0094, -0.0613, -0.0355,  0.0585, -0.0152,  0.0693, -0.0634,  0.0654,\n",
      "        -0.0077, -0.0618,  0.0434, -0.0529, -0.0532,  0.0226,  0.0866,  0.0246,\n",
      "         0.0698,  0.0548,  0.0021, -0.0593,  0.0701, -0.0504, -0.0541,  0.0058,\n",
      "        -0.0900, -0.0420, -0.0294,  0.0304, -0.0377,  0.0482,  0.0212, -0.0523,\n",
      "         0.0753, -0.0668,  0.0050,  0.0224,  0.0524, -0.0897, -0.0039,  0.0163,\n",
      "        -0.0303, -0.0873, -0.0179, -0.0721], requires_grad=True)\n",
      "before Parameter containing:\n",
      "tensor([[ 2.0953e-02,  5.4121e-02,  5.9873e-02, -9.0797e-02, -6.6627e-03,\n",
      "         -9.8816e-02,  8.4101e-03, -3.9009e-02, -3.4104e-02,  1.0481e-01,\n",
      "          7.7701e-02, -2.4384e-02,  5.6395e-02,  7.5572e-03,  7.9388e-02,\n",
      "          1.0860e-01,  4.7899e-02,  1.0073e-01,  5.7307e-02, -5.3392e-02,\n",
      "          2.8453e-02, -1.0664e-01, -6.1918e-02, -4.0296e-02, -6.2328e-02,\n",
      "          1.8480e-02, -7.3693e-02, -6.8913e-02, -3.7547e-02,  7.9956e-02,\n",
      "         -9.6200e-02,  3.7393e-02, -8.7848e-03,  4.7863e-03,  4.1682e-02,\n",
      "         -8.5610e-02, -4.6450e-02,  1.0874e-01,  4.8631e-02, -3.7064e-02,\n",
      "         -3.9428e-02, -9.8804e-02,  9.7269e-02,  1.0070e-01,  5.5906e-03,\n",
      "          7.6701e-02, -7.7196e-02,  9.3929e-02, -9.9134e-02,  8.2097e-02,\n",
      "          4.5488e-02,  5.0606e-02, -6.1575e-04,  4.0372e-03, -7.7042e-02,\n",
      "          5.5907e-02,  7.9824e-02,  6.2278e-02,  6.9001e-02, -2.9047e-03,\n",
      "          8.6139e-02,  1.1681e-02, -4.2792e-02,  7.8416e-02,  6.2983e-02,\n",
      "          2.1290e-02,  6.2676e-02, -1.0787e-01,  8.8579e-02,  8.5869e-02,\n",
      "         -2.1305e-02,  1.6007e-03, -9.6162e-02,  7.4064e-02, -7.9969e-02,\n",
      "         -2.0782e-02,  1.0817e-01,  4.9324e-02,  9.0045e-02, -9.5113e-02,\n",
      "          7.5274e-02, -6.3220e-02, -8.0452e-02, -6.3676e-02],\n",
      "        [ 6.8128e-02,  7.7672e-02,  1.0253e-01, -9.3183e-03,  4.4703e-02,\n",
      "         -9.9436e-02, -1.1148e-02,  5.6658e-02, -1.0781e-01,  4.4705e-02,\n",
      "          9.4345e-02,  1.0528e-01,  2.9884e-02,  6.2878e-02, -8.0808e-02,\n",
      "          4.3481e-02, -5.5190e-02, -8.0049e-02,  4.7243e-02,  1.3565e-02,\n",
      "          7.2783e-02,  4.6852e-02,  5.5810e-02,  7.7044e-02,  3.2334e-02,\n",
      "         -3.9787e-02, -2.0330e-02, -3.2681e-02, -1.0806e-01,  4.3038e-02,\n",
      "         -7.1936e-02,  9.8946e-02,  1.0315e-01, -9.4102e-02, -2.4415e-02,\n",
      "          7.4485e-02,  9.5380e-02, -8.5339e-02, -7.2622e-02, -2.1966e-02,\n",
      "         -8.7577e-02,  8.8749e-02,  7.8476e-02, -3.2179e-02, -8.5317e-02,\n",
      "         -8.9730e-02,  2.0294e-02,  6.2397e-02, -9.5098e-02,  7.0127e-02,\n",
      "         -2.0187e-02, -4.1332e-02, -1.0502e-01,  7.7041e-02, -6.7392e-02,\n",
      "         -5.5587e-03, -5.1824e-02, -3.8265e-02,  7.6085e-02, -4.1824e-03,\n",
      "         -8.3337e-02,  1.3092e-02, -1.0104e-01,  2.7131e-02, -1.5044e-02,\n",
      "         -1.2977e-02, -5.5645e-02,  6.5191e-02, -6.0121e-02, -1.4623e-02,\n",
      "          9.7776e-03, -3.7785e-02,  5.5605e-03,  8.3212e-02, -7.0903e-02,\n",
      "          4.1796e-02,  5.9237e-02, -9.4390e-02,  5.9619e-02,  1.0058e-01,\n",
      "         -8.0567e-03, -6.2809e-02, -3.7234e-02,  7.6980e-02],\n",
      "        [-8.2487e-02, -9.8362e-02,  9.9782e-02, -9.0612e-02, -5.4401e-02,\n",
      "          2.4558e-02, -9.1821e-02,  1.0092e-01,  8.6361e-02, -1.1594e-02,\n",
      "          6.8561e-02, -5.7102e-02,  3.0160e-02, -6.6162e-02,  9.7298e-02,\n",
      "         -6.1867e-02,  1.2447e-02, -8.2024e-02, -1.0767e-01, -4.4117e-03,\n",
      "         -4.1746e-04,  7.2339e-02, -6.8868e-02, -7.9283e-02, -6.5830e-03,\n",
      "         -6.4609e-02,  5.7505e-02,  4.4259e-02,  9.2012e-02,  4.8650e-02,\n",
      "          7.3759e-02, -1.0289e-01,  7.3550e-02, -3.7730e-02, -7.9579e-03,\n",
      "          4.0272e-02,  6.3265e-02, -5.1555e-02, -7.5577e-02,  6.9042e-02,\n",
      "         -3.9959e-02,  1.7209e-02, -4.6158e-02,  7.2386e-02,  3.7871e-02,\n",
      "         -2.9354e-02,  3.0898e-02, -9.7364e-02, -8.9507e-02, -7.8254e-02,\n",
      "          2.2860e-02,  1.0356e-01,  3.8484e-02,  3.1044e-02, -8.5931e-02,\n",
      "          1.9908e-02,  3.1577e-02, -5.8324e-03,  3.8285e-02,  1.0104e-01,\n",
      "          2.8018e-02,  7.2669e-02,  8.4870e-02, -2.2892e-02, -8.2326e-02,\n",
      "          5.9229e-02, -5.2017e-02,  1.8200e-02,  4.0352e-02,  6.3023e-02,\n",
      "          3.5526e-02,  6.3034e-02, -3.8035e-02,  9.1449e-02, -5.2227e-02,\n",
      "         -4.6430e-02, -2.6947e-02,  4.6453e-02, -5.8218e-03,  6.4163e-02,\n",
      "          5.3778e-02,  4.3599e-02, -2.4658e-02, -1.0448e-01],\n",
      "        [ 6.8151e-02,  3.7250e-02, -7.2480e-03,  8.9739e-02,  3.9316e-02,\n",
      "         -2.9477e-02,  6.3814e-02, -9.6388e-02, -4.7808e-02,  2.7074e-02,\n",
      "         -5.4989e-02,  2.5827e-02, -1.2789e-02, -2.6686e-02, -6.9061e-02,\n",
      "         -5.2104e-02, -7.0050e-02, -9.5186e-02,  4.1933e-02, -2.7001e-02,\n",
      "          4.9971e-02, -2.8263e-02, -7.2927e-02,  3.7785e-02,  6.2832e-02,\n",
      "          1.5370e-02, -4.7993e-02, -5.5713e-02, -1.6633e-02, -3.9037e-02,\n",
      "         -2.2794e-02,  1.1587e-02, -5.1313e-02, -6.4644e-02, -3.8252e-02,\n",
      "         -1.2477e-02,  5.5593e-02, -2.3905e-02,  3.8694e-02, -2.1639e-02,\n",
      "          7.2395e-02, -8.2337e-02,  6.8110e-02, -6.1200e-02,  1.3497e-02,\n",
      "         -5.8707e-02, -8.3226e-02,  6.8469e-02, -1.0483e-01,  6.3863e-02,\n",
      "         -5.2125e-02, -8.8416e-02, -1.3703e-02, -4.0296e-02, -3.7617e-02,\n",
      "          6.9897e-02, -1.2162e-02,  2.2385e-02,  1.0902e-01,  6.3553e-02,\n",
      "         -8.9658e-02, -6.2232e-02,  6.2826e-02,  1.4039e-02,  9.3148e-02,\n",
      "          4.6753e-04,  8.5028e-02, -9.4181e-02, -2.3014e-02, -2.3944e-03,\n",
      "          5.4088e-02, -7.5667e-02,  3.1989e-02,  7.6733e-02, -4.2482e-02,\n",
      "          4.5686e-02, -5.1582e-02, -3.0028e-02, -4.3103e-02, -1.1514e-02,\n",
      "          4.2591e-02,  1.4038e-02, -6.5526e-02, -4.6166e-02],\n",
      "        [-2.8103e-02,  8.7401e-02,  4.5018e-02,  7.3832e-02, -1.0297e-01,\n",
      "          5.7965e-02,  7.6928e-02,  4.9744e-02, -2.5466e-02,  6.5712e-02,\n",
      "          8.0194e-02,  2.8787e-02,  3.0753e-02,  5.4921e-02, -5.0652e-02,\n",
      "         -5.7139e-02,  9.9047e-02, -4.3205e-02, -6.7390e-02,  6.4124e-02,\n",
      "         -1.5570e-02,  9.9463e-02, -1.0904e-01, -7.5515e-02,  5.0538e-02,\n",
      "         -9.2032e-03, -3.5625e-02,  1.9180e-02,  1.5611e-02, -1.0583e-02,\n",
      "          4.7053e-02,  5.4903e-02, -3.1688e-02,  9.1534e-02,  6.8125e-02,\n",
      "         -4.1876e-02, -8.8281e-02, -4.9309e-02,  9.0858e-02, -1.0118e-01,\n",
      "         -6.9648e-02,  8.2402e-02,  3.1533e-02,  5.9064e-02, -9.2757e-02,\n",
      "         -5.1535e-03, -9.7705e-02,  1.0739e-01,  1.9443e-02, -1.0939e-02,\n",
      "         -4.0725e-02, -3.4272e-02,  9.7021e-02,  2.2613e-02, -4.5387e-02,\n",
      "         -1.0578e-01,  4.2508e-02, -9.0024e-02, -3.2240e-02,  9.0153e-02,\n",
      "         -1.0582e-01, -9.6685e-02, -5.8797e-02,  8.6731e-02, -2.1022e-02,\n",
      "         -1.7611e-02, -7.2088e-02, -5.0218e-02,  4.8468e-02, -4.5987e-02,\n",
      "         -9.8239e-02,  7.8785e-03,  7.4837e-02, -6.8509e-02,  8.2393e-02,\n",
      "         -9.8508e-03,  3.3537e-02, -4.2101e-03,  1.7444e-02, -3.9233e-02,\n",
      "         -8.2297e-02,  4.7472e-02, -9.9878e-02,  4.3610e-02],\n",
      "        [ 1.2800e-02, -9.8816e-02,  2.2226e-02, -2.1994e-05,  6.9279e-02,\n",
      "         -1.5590e-02, -1.0132e-01,  5.9550e-02,  4.7868e-02,  7.4608e-02,\n",
      "          5.8665e-02,  7.5059e-02, -4.0558e-02, -8.5275e-02,  6.4781e-02,\n",
      "         -1.2180e-02,  1.0624e-01,  4.8440e-02, -3.3295e-02, -5.4397e-02,\n",
      "         -8.2286e-02, -4.7835e-02,  1.6216e-02,  4.5109e-02,  7.9348e-02,\n",
      "          8.4872e-02,  3.1425e-02,  4.8607e-02, -3.0002e-02,  8.6723e-03,\n",
      "         -5.7726e-02, -1.1792e-02,  5.7814e-02, -1.0428e-01, -6.2389e-02,\n",
      "         -7.3484e-02, -9.1342e-03, -4.2253e-02,  8.5267e-02, -2.1170e-02,\n",
      "          3.2438e-02, -2.3360e-02, -8.3456e-02, -7.9250e-02, -8.7457e-02,\n",
      "          8.0485e-02, -8.3153e-02, -5.5065e-03, -4.5762e-02, -5.5210e-02,\n",
      "         -1.0836e-01, -8.3390e-02, -2.7090e-02,  8.0051e-02, -3.7079e-02,\n",
      "         -6.0931e-02,  2.8762e-02,  7.0372e-03,  9.4191e-02,  2.1895e-02,\n",
      "         -9.7528e-02, -4.3010e-02, -7.1123e-02, -7.1594e-02,  1.5121e-03,\n",
      "         -2.0351e-02, -5.1433e-03, -2.4055e-02, -6.9548e-02, -3.7836e-02,\n",
      "          6.5448e-02, -3.8343e-02, -6.2018e-02, -1.1412e-03, -9.6810e-02,\n",
      "         -9.2508e-02, -4.9141e-02,  6.8448e-02,  1.0586e-01, -2.8158e-02,\n",
      "         -8.0537e-02, -1.1284e-02,  9.9301e-02, -6.0437e-02],\n",
      "        [ 7.5258e-02,  2.1107e-02, -4.0757e-02,  6.2520e-02, -2.7241e-02,\n",
      "          8.7919e-02,  1.9819e-02, -3.9864e-02, -8.1504e-02, -2.4375e-02,\n",
      "          1.0005e-01,  4.1215e-02,  6.1174e-02,  6.7134e-02,  9.2614e-02,\n",
      "         -8.9097e-02, -6.4134e-02,  9.2849e-02, -8.1549e-02, -4.4194e-02,\n",
      "          1.1090e-02,  6.0302e-02, -9.3671e-02, -3.9311e-02, -7.2856e-02,\n",
      "          1.0691e-01,  9.7805e-02, -9.1875e-02, -5.6271e-02, -6.1387e-02,\n",
      "          9.9494e-02,  4.0289e-02,  6.3453e-02, -1.0485e-01,  1.5575e-02,\n",
      "         -1.2094e-02,  1.0254e-01,  6.8041e-02, -2.8371e-02, -5.0162e-02,\n",
      "         -2.0251e-02,  9.7145e-02, -5.0705e-02,  8.9955e-03, -7.1797e-02,\n",
      "          8.7579e-02, -9.1174e-02, -9.4249e-03,  4.6715e-02,  4.5504e-02,\n",
      "         -9.6142e-02, -9.3586e-02,  1.7627e-02, -4.9152e-02, -9.6429e-02,\n",
      "          7.4630e-02,  4.7022e-02, -3.6957e-02, -6.9514e-02, -5.5807e-02,\n",
      "          1.0800e-01,  4.5223e-02, -2.9414e-02,  3.4618e-03,  6.3579e-03,\n",
      "         -2.2169e-02,  2.3249e-02, -3.5589e-02,  9.5300e-02, -2.4043e-02,\n",
      "         -4.5684e-02, -4.0266e-02, -5.2230e-02, -4.8066e-02,  1.0358e-01,\n",
      "          3.5552e-02,  1.0402e-01, -9.5948e-02,  2.4190e-02,  8.1651e-02,\n",
      "          5.9559e-02, -3.9963e-02,  9.4982e-02, -8.8172e-02],\n",
      "        [-8.6967e-02,  3.4623e-03, -8.9810e-02,  4.5742e-02, -1.0134e-01,\n",
      "         -8.8128e-02, -1.0415e-02,  2.0954e-02, -4.1599e-02, -7.0265e-02,\n",
      "          1.5003e-02, -4.4249e-02,  6.3868e-02, -4.9764e-02,  4.6829e-02,\n",
      "         -2.5371e-02,  6.2097e-02,  6.8981e-03, -7.0622e-02,  4.4865e-02,\n",
      "         -7.3778e-02,  4.1760e-02, -5.9426e-02,  8.5172e-02, -2.0350e-02,\n",
      "          7.0633e-02, -8.6983e-02,  1.0594e-01,  8.1596e-02,  9.8957e-02,\n",
      "         -8.1292e-03, -6.2108e-02, -9.3754e-02,  1.0400e-01,  1.6667e-02,\n",
      "         -7.1623e-02, -8.7303e-02,  3.7647e-02,  4.8567e-02, -2.8134e-02,\n",
      "         -2.8931e-03,  8.1659e-02,  1.4520e-02, -4.6719e-02, -1.9391e-02,\n",
      "         -3.7826e-02, -8.3805e-02, -7.8138e-02,  9.2673e-02,  3.9743e-02,\n",
      "          1.0170e-01, -4.6563e-02,  1.0315e-01,  2.8535e-02, -5.3179e-02,\n",
      "         -6.1218e-02,  9.1601e-03,  5.0316e-02,  6.5542e-02, -4.8077e-02,\n",
      "         -5.9167e-02,  3.3744e-02, -7.2285e-02, -9.3909e-02, -2.5816e-02,\n",
      "         -7.6105e-02,  2.4402e-02,  5.3649e-02, -7.8315e-03, -1.1438e-02,\n",
      "          5.9462e-03, -4.7857e-02, -6.9855e-02,  9.7601e-02, -2.5640e-03,\n",
      "          1.8649e-02, -4.2238e-03,  9.4558e-04,  6.6262e-02, -8.1728e-02,\n",
      "          6.5508e-02, -3.9159e-02, -8.8291e-02, -8.7318e-02],\n",
      "        [-2.7188e-03,  5.4367e-02,  5.1590e-02, -1.3562e-02,  8.9866e-02,\n",
      "          4.6389e-02, -2.1591e-02,  9.3692e-02, -1.0739e-01,  1.0809e-01,\n",
      "         -8.7025e-02,  3.9028e-02, -4.8584e-02, -6.4579e-02,  7.5554e-02,\n",
      "          3.5307e-02, -9.7070e-03,  6.7158e-02,  8.4677e-02,  9.1933e-02,\n",
      "          4.0427e-02,  1.0316e-01,  4.1468e-02, -1.0070e-01,  2.0095e-03,\n",
      "         -9.9876e-02, -8.3914e-02,  9.6895e-02, -8.8748e-02, -7.4745e-02,\n",
      "          1.0361e-03, -9.0867e-02,  7.5746e-02,  1.0573e-01,  4.1811e-02,\n",
      "          4.3259e-04,  9.6483e-02,  5.3404e-02, -2.0782e-02, -4.3929e-02,\n",
      "         -1.0469e-01, -3.4329e-02,  3.5204e-02, -4.6629e-02,  1.0081e-01,\n",
      "         -6.3040e-02, -1.0441e-02,  1.4102e-02, -7.3835e-02, -9.7545e-02,\n",
      "          1.4458e-02,  1.0602e-01,  3.7415e-02, -6.6517e-02, -3.8192e-03,\n",
      "         -4.3580e-02,  9.1921e-03,  4.3051e-02,  5.6436e-02, -9.7258e-04,\n",
      "         -3.5893e-03, -1.0145e-01,  1.6567e-02, -5.8068e-02,  1.6679e-02,\n",
      "         -4.8920e-02,  9.0950e-02,  6.8081e-03, -7.4133e-02, -1.5954e-02,\n",
      "          1.0645e-01,  1.8322e-02, -9.3462e-02, -1.0705e-01,  2.7913e-02,\n",
      "         -6.3384e-02,  6.5441e-02,  1.0004e-01, -1.0272e-01,  4.8898e-02,\n",
      "         -1.0551e-01, -1.8998e-02, -1.0893e-01, -6.3354e-02],\n",
      "        [-9.0362e-02, -1.9945e-02, -5.0825e-02,  3.0411e-02, -6.5037e-02,\n",
      "         -1.0456e-01, -7.5965e-02, -1.1769e-02, -5.1150e-02, -4.6795e-02,\n",
      "         -3.7749e-02,  7.3686e-02,  5.3418e-03, -9.0606e-02,  3.6061e-02,\n",
      "          4.1710e-02, -2.0622e-04, -9.5983e-02, -9.3009e-02,  9.0252e-02,\n",
      "         -7.5973e-02, -6.0311e-02, -5.4521e-02,  1.0822e-01,  1.5134e-02,\n",
      "         -3.6566e-03,  9.7133e-02,  8.1424e-03, -6.9964e-02,  3.2420e-02,\n",
      "         -9.0516e-02, -1.6454e-02,  5.6232e-02,  4.7381e-02, -1.6550e-02,\n",
      "          4.0839e-02, -6.7715e-02, -7.1883e-02,  6.7932e-02,  2.2013e-02,\n",
      "         -1.2654e-02, -4.2104e-02,  2.0633e-02,  8.4022e-02,  6.6383e-02,\n",
      "          9.0408e-02,  7.1293e-02, -9.8637e-03, -7.0690e-02,  6.1616e-02,\n",
      "         -6.9894e-02,  9.0017e-02,  6.0281e-02, -5.1005e-02, -4.1846e-02,\n",
      "         -9.1111e-02, -4.4838e-02, -2.0705e-02, -1.0271e-01,  2.0146e-02,\n",
      "          1.7735e-02,  1.7627e-02,  1.0500e-01,  1.9043e-02,  7.3499e-02,\n",
      "         -8.3199e-02,  5.9128e-02, -1.0726e-01, -9.0144e-02, -5.3699e-02,\n",
      "         -3.8767e-02, -1.7590e-02,  4.6546e-02, -1.0669e-01, -5.0664e-02,\n",
      "         -9.4486e-02, -5.8548e-02,  1.1343e-02,  3.0969e-02, -3.8233e-02,\n",
      "          1.0014e-01, -6.8462e-02, -2.7425e-02, -5.9172e-02]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([[ 2.0953e-02,  5.4121e-02,  5.9873e-02, -9.0797e-02, -6.6533e-03,\n",
      "         -9.8816e-02,  8.4101e-03, -3.9009e-02, -3.4080e-02,  1.0481e-01,\n",
      "          7.7714e-02, -2.4384e-02,  5.6395e-02,  7.5572e-03,  7.9388e-02,\n",
      "          1.0860e-01,  4.7899e-02,  1.0073e-01,  5.7370e-02, -5.3392e-02,\n",
      "          2.8453e-02, -1.0658e-01, -6.1918e-02, -4.0296e-02, -6.2328e-02,\n",
      "          1.8533e-02, -7.3682e-02, -6.8913e-02, -3.7547e-02,  7.9958e-02,\n",
      "         -9.6199e-02,  3.7411e-02, -8.7319e-03,  4.8241e-03,  4.1737e-02,\n",
      "         -8.5574e-02, -4.6442e-02,  1.0874e-01,  4.8666e-02, -3.7064e-02,\n",
      "         -3.9428e-02, -9.8783e-02,  9.7269e-02,  1.0070e-01,  5.6409e-03,\n",
      "          7.6701e-02, -7.7196e-02,  9.3929e-02, -9.9091e-02,  8.2097e-02,\n",
      "          4.5548e-02,  5.0606e-02, -6.1575e-04,  4.0498e-03, -7.7022e-02,\n",
      "          5.5953e-02,  7.9834e-02,  6.2282e-02,  6.9019e-02, -2.8734e-03,\n",
      "          8.6267e-02,  1.1681e-02, -4.2713e-02,  7.8421e-02,  6.2983e-02,\n",
      "          2.1322e-02,  6.2676e-02, -1.0782e-01,  8.8579e-02,  8.5869e-02,\n",
      "         -2.1305e-02,  1.6007e-03, -9.6162e-02,  7.4065e-02, -7.9899e-02,\n",
      "         -2.0763e-02,  1.0824e-01,  4.9324e-02,  9.0089e-02, -9.5113e-02,\n",
      "          7.5274e-02, -6.3220e-02, -8.0452e-02, -6.3676e-02],\n",
      "        [ 6.8128e-02,  7.7672e-02,  1.0253e-01, -9.3183e-03,  4.4674e-02,\n",
      "         -9.9436e-02, -1.1148e-02,  5.6658e-02, -1.0789e-01,  4.4705e-02,\n",
      "          9.4305e-02,  1.0528e-01,  2.9884e-02,  6.2878e-02, -8.0808e-02,\n",
      "          4.3481e-02, -5.5190e-02, -8.0049e-02,  4.7052e-02,  1.3565e-02,\n",
      "          7.2783e-02,  4.6688e-02,  5.5810e-02,  7.7044e-02,  3.2334e-02,\n",
      "         -3.9947e-02, -2.0363e-02, -3.2681e-02, -1.0806e-01,  4.3032e-02,\n",
      "         -7.1940e-02,  9.8894e-02,  1.0299e-01, -9.4216e-02, -2.4580e-02,\n",
      "          7.4376e-02,  9.5353e-02, -8.5339e-02, -7.2729e-02, -2.1966e-02,\n",
      "         -8.7577e-02,  8.8685e-02,  7.8476e-02, -3.2179e-02, -8.5469e-02,\n",
      "         -8.9730e-02,  2.0294e-02,  6.2397e-02, -9.5226e-02,  7.0127e-02,\n",
      "         -2.0366e-02, -4.1332e-02, -1.0502e-01,  7.7002e-02, -6.7451e-02,\n",
      "         -5.6954e-03, -5.1854e-02, -3.8278e-02,  7.6031e-02, -4.2764e-03,\n",
      "         -8.3723e-02,  1.3092e-02, -1.0128e-01,  2.7119e-02, -1.5044e-02,\n",
      "         -1.3071e-02, -5.5645e-02,  6.5047e-02, -6.0121e-02, -1.4623e-02,\n",
      "          9.7776e-03, -3.7785e-02,  5.5605e-03,  8.3210e-02, -7.1113e-02,\n",
      "          4.1738e-02,  5.9040e-02, -9.4390e-02,  5.9488e-02,  1.0058e-01,\n",
      "         -8.0567e-03, -6.2809e-02, -3.7234e-02,  7.6980e-02],\n",
      "        [-8.2487e-02, -9.8362e-02,  9.9782e-02, -9.0612e-02, -5.4349e-02,\n",
      "          2.4558e-02, -9.1821e-02,  1.0092e-01,  8.6493e-02, -1.1594e-02,\n",
      "          6.8634e-02, -5.7102e-02,  3.0160e-02, -6.6162e-02,  9.7298e-02,\n",
      "         -6.1867e-02,  1.2447e-02, -8.2024e-02, -1.0732e-01, -4.4117e-03,\n",
      "         -4.1746e-04,  7.2637e-02, -6.8868e-02, -7.9283e-02, -6.5830e-03,\n",
      "         -6.4318e-02,  5.7566e-02,  4.4259e-02,  9.2012e-02,  4.8660e-02,\n",
      "          7.3765e-02, -1.0280e-01,  7.3840e-02, -3.7523e-02, -7.6585e-03,\n",
      "          4.0470e-02,  6.3313e-02, -5.1555e-02, -7.5381e-02,  6.9042e-02,\n",
      "         -3.9959e-02,  1.7324e-02, -4.6158e-02,  7.2386e-02,  3.8146e-02,\n",
      "         -2.9354e-02,  3.0898e-02, -9.7364e-02, -8.9274e-02, -7.8254e-02,\n",
      "          2.3186e-02,  1.0356e-01,  3.8484e-02,  3.1114e-02, -8.5824e-02,\n",
      "          2.0156e-02,  3.1632e-02, -5.8085e-03,  3.8384e-02,  1.0121e-01,\n",
      "          2.8721e-02,  7.2669e-02,  8.5304e-02, -2.2870e-02, -8.2326e-02,\n",
      "          5.9400e-02, -5.2017e-02,  1.8462e-02,  4.0352e-02,  6.3023e-02,\n",
      "          3.5526e-02,  6.3034e-02, -3.8035e-02,  9.1453e-02, -5.1845e-02,\n",
      "         -4.6325e-02, -2.6589e-02,  4.6453e-02, -5.5829e-03,  6.4163e-02,\n",
      "          5.3778e-02,  4.3599e-02, -2.4658e-02, -1.0448e-01],\n",
      "        [ 6.8151e-02,  3.7250e-02, -7.2480e-03,  8.9739e-02,  3.9330e-02,\n",
      "         -2.9477e-02,  6.3814e-02, -9.6388e-02, -4.7774e-02,  2.7074e-02,\n",
      "         -5.4970e-02,  2.5827e-02, -1.2789e-02, -2.6686e-02, -6.9061e-02,\n",
      "         -5.2104e-02, -7.0050e-02, -9.5186e-02,  4.2024e-02, -2.7001e-02,\n",
      "          4.9971e-02, -2.8185e-02, -7.2927e-02,  3.7785e-02,  6.2832e-02,\n",
      "          1.5446e-02, -4.7977e-02, -5.5713e-02, -1.6633e-02, -3.9034e-02,\n",
      "         -2.2792e-02,  1.1612e-02, -5.1237e-02, -6.4590e-02, -3.8174e-02,\n",
      "         -1.2425e-02,  5.5605e-02, -2.3905e-02,  3.8745e-02, -2.1639e-02,\n",
      "          7.2395e-02, -8.2307e-02,  6.8110e-02, -6.1200e-02,  1.3569e-02,\n",
      "         -5.8707e-02, -8.3226e-02,  6.8469e-02, -1.0477e-01,  6.3863e-02,\n",
      "         -5.2040e-02, -8.8416e-02, -1.3703e-02, -4.0278e-02, -3.7589e-02,\n",
      "          6.9962e-02, -1.2148e-02,  2.2391e-02,  1.0905e-01,  6.3598e-02,\n",
      "         -8.9474e-02, -6.2232e-02,  6.2939e-02,  1.4044e-02,  9.3148e-02,\n",
      "          5.1221e-04,  8.5028e-02, -9.4112e-02, -2.3014e-02, -2.3944e-03,\n",
      "          5.4088e-02, -7.5667e-02,  3.1989e-02,  7.6734e-02, -4.2382e-02,\n",
      "          4.5714e-02, -5.1488e-02, -3.0028e-02, -4.3041e-02, -1.1514e-02,\n",
      "          4.2591e-02,  1.4038e-02, -6.5526e-02, -4.6166e-02],\n",
      "        [-2.8103e-02,  8.7401e-02,  4.5018e-02,  7.3832e-02, -1.0297e-01,\n",
      "          5.7965e-02,  7.6928e-02,  4.9744e-02, -2.5487e-02,  6.5712e-02,\n",
      "          8.0182e-02,  2.8787e-02,  3.0753e-02,  5.4921e-02, -5.0652e-02,\n",
      "         -5.7139e-02,  9.9047e-02, -4.3205e-02, -6.7447e-02,  6.4124e-02,\n",
      "         -1.5570e-02,  9.9414e-02, -1.0904e-01, -7.5515e-02,  5.0538e-02,\n",
      "         -9.2507e-03, -3.5635e-02,  1.9180e-02,  1.5611e-02, -1.0584e-02,\n",
      "          4.7052e-02,  5.4888e-02, -3.1735e-02,  9.1500e-02,  6.8077e-02,\n",
      "         -4.1908e-02, -8.8288e-02, -4.9309e-02,  9.0826e-02, -1.0118e-01,\n",
      "         -6.9648e-02,  8.2383e-02,  3.1533e-02,  5.9064e-02, -9.2802e-02,\n",
      "         -5.1535e-03, -9.7705e-02,  1.0739e-01,  1.9405e-02, -1.0939e-02,\n",
      "         -4.0778e-02, -3.4272e-02,  9.7021e-02,  2.2602e-02, -4.5404e-02,\n",
      "         -1.0582e-01,  4.2499e-02, -9.0028e-02, -3.2256e-02,  9.0125e-02,\n",
      "         -1.0593e-01, -9.6685e-02, -5.8867e-02,  8.6727e-02, -2.1022e-02,\n",
      "         -1.7639e-02, -7.2088e-02, -5.0261e-02,  4.8468e-02, -4.5987e-02,\n",
      "         -9.8239e-02,  7.8785e-03,  7.4837e-02, -6.8510e-02,  8.2331e-02,\n",
      "         -9.8679e-03,  3.3479e-02, -4.2101e-03,  1.7405e-02, -3.9233e-02,\n",
      "         -8.2297e-02,  4.7472e-02, -9.9878e-02,  4.3610e-02],\n",
      "        [ 1.2800e-02, -9.8816e-02,  2.2226e-02, -2.1994e-05,  6.9291e-02,\n",
      "         -1.5590e-02, -1.0132e-01,  5.9550e-02,  4.7898e-02,  7.4608e-02,\n",
      "          5.8682e-02,  7.5059e-02, -4.0558e-02, -8.5275e-02,  6.4781e-02,\n",
      "         -1.2180e-02,  1.0624e-01,  4.8440e-02, -3.3214e-02, -5.4397e-02,\n",
      "         -8.2286e-02, -4.7766e-02,  1.6216e-02,  4.5109e-02,  7.9348e-02,\n",
      "          8.4939e-02,  3.1439e-02,  4.8607e-02, -3.0002e-02,  8.6747e-03,\n",
      "         -5.7725e-02, -1.1770e-02,  5.7881e-02, -1.0424e-01, -6.2320e-02,\n",
      "         -7.3438e-02, -9.1231e-03, -4.2253e-02,  8.5312e-02, -2.1170e-02,\n",
      "          3.2438e-02, -2.3334e-02, -8.3456e-02, -7.9250e-02, -8.7393e-02,\n",
      "          8.0485e-02, -8.3153e-02, -5.5065e-03, -4.5708e-02, -5.5210e-02,\n",
      "         -1.0828e-01, -8.3390e-02, -2.7090e-02,  8.0067e-02, -3.7054e-02,\n",
      "         -6.0874e-02,  2.8774e-02,  7.0427e-03,  9.4214e-02,  2.1934e-02,\n",
      "         -9.7365e-02, -4.3010e-02, -7.1023e-02, -7.1589e-02,  1.5121e-03,\n",
      "         -2.0312e-02, -5.1433e-03, -2.3994e-02, -6.9548e-02, -3.7836e-02,\n",
      "          6.5448e-02, -3.8343e-02, -6.2018e-02, -1.1404e-03, -9.6721e-02,\n",
      "         -9.2483e-02, -4.9058e-02,  6.8448e-02,  1.0591e-01, -2.8158e-02,\n",
      "         -8.0537e-02, -1.1284e-02,  9.9301e-02, -6.0437e-02],\n",
      "        [ 7.5258e-02,  2.1107e-02, -4.0757e-02,  6.2520e-02, -2.7295e-02,\n",
      "          8.7919e-02,  1.9819e-02, -3.9864e-02, -8.1641e-02, -2.4375e-02,\n",
      "          9.9970e-02,  4.1215e-02,  6.1174e-02,  6.7134e-02,  9.2614e-02,\n",
      "         -8.9097e-02, -6.4134e-02,  9.2849e-02, -8.1909e-02, -4.4194e-02,\n",
      "          1.1090e-02,  5.9994e-02, -9.3671e-02, -3.9311e-02, -7.2856e-02,\n",
      "          1.0661e-01,  9.7742e-02, -9.1875e-02, -5.6271e-02, -6.1398e-02,\n",
      "          9.9488e-02,  4.0191e-02,  6.3153e-02, -1.0506e-01,  1.5265e-02,\n",
      "         -1.2299e-02,  1.0249e-01,  6.8041e-02, -2.8573e-02, -5.0162e-02,\n",
      "         -2.0251e-02,  9.7026e-02, -5.0705e-02,  8.9955e-03, -7.2082e-02,\n",
      "          8.7579e-02, -9.1174e-02, -9.4249e-03,  4.6473e-02,  4.5504e-02,\n",
      "         -9.6480e-02, -9.3586e-02,  1.7627e-02, -4.9224e-02, -9.6540e-02,\n",
      "          7.4372e-02,  4.6966e-02, -3.6982e-02, -6.9616e-02, -5.5984e-02,\n",
      "          1.0728e-01,  4.5223e-02, -2.9863e-02,  3.4389e-03,  6.3579e-03,\n",
      "         -2.2346e-02,  2.3249e-02, -3.5860e-02,  9.5300e-02, -2.4043e-02,\n",
      "         -4.5684e-02, -4.0266e-02, -5.2230e-02, -4.8070e-02,  1.0318e-01,\n",
      "          3.5443e-02,  1.0365e-01, -9.5948e-02,  2.3942e-02,  8.1651e-02,\n",
      "          5.9559e-02, -3.9963e-02,  9.4982e-02, -8.8172e-02],\n",
      "        [-8.6967e-02,  3.4623e-03, -8.9810e-02,  4.5742e-02, -1.0132e-01,\n",
      "         -8.8128e-02, -1.0415e-02,  2.0954e-02, -4.1543e-02, -7.0265e-02,\n",
      "          1.5034e-02, -4.4249e-02,  6.3868e-02, -4.9764e-02,  4.6829e-02,\n",
      "         -2.5371e-02,  6.2097e-02,  6.8981e-03, -7.0475e-02,  4.4865e-02,\n",
      "         -7.3778e-02,  4.1886e-02, -5.9426e-02,  8.5172e-02, -2.0350e-02,\n",
      "          7.0756e-02, -8.6958e-02,  1.0594e-01,  8.1596e-02,  9.8962e-02,\n",
      "         -8.1268e-03, -6.2068e-02, -9.3632e-02,  1.0409e-01,  1.6793e-02,\n",
      "         -7.1540e-02, -8.7283e-02,  3.7647e-02,  4.8649e-02, -2.8134e-02,\n",
      "         -2.8931e-03,  8.1708e-02,  1.4520e-02, -4.6719e-02, -1.9275e-02,\n",
      "         -3.7826e-02, -8.3805e-02, -7.8138e-02,  9.2771e-02,  3.9743e-02,\n",
      "          1.0183e-01, -4.6563e-02,  1.0315e-01,  2.8564e-02, -5.3134e-02,\n",
      "         -6.1113e-02,  9.1832e-03,  5.0326e-02,  6.5583e-02, -4.8005e-02,\n",
      "         -5.8870e-02,  3.3744e-02, -7.2102e-02, -9.3900e-02, -2.5816e-02,\n",
      "         -7.6033e-02,  2.4402e-02,  5.3759e-02, -7.8315e-03, -1.1438e-02,\n",
      "          5.9462e-03, -4.7857e-02, -6.9855e-02,  9.7602e-02, -2.4030e-03,\n",
      "          1.8694e-02, -4.0729e-03,  9.4558e-04,  6.6362e-02, -8.1728e-02,\n",
      "          6.5508e-02, -3.9159e-02, -8.8291e-02, -8.7318e-02],\n",
      "        [-2.7188e-03,  5.4367e-02,  5.1590e-02, -1.3562e-02,  8.9890e-02,\n",
      "          4.6389e-02, -2.1591e-02,  9.3692e-02, -1.0733e-01,  1.0809e-01,\n",
      "         -8.6991e-02,  3.9028e-02, -4.8584e-02, -6.4579e-02,  7.5554e-02,\n",
      "          3.5307e-02, -9.7070e-03,  6.7158e-02,  8.4836e-02,  9.1933e-02,\n",
      "          4.0427e-02,  1.0329e-01,  4.1468e-02, -1.0070e-01,  2.0095e-03,\n",
      "         -9.9742e-02, -8.3886e-02,  9.6895e-02, -8.8748e-02, -7.4740e-02,\n",
      "          1.0387e-03, -9.0823e-02,  7.5879e-02,  1.0582e-01,  4.1947e-02,\n",
      "          5.2319e-04,  9.6505e-02,  5.3404e-02, -2.0693e-02, -4.3929e-02,\n",
      "         -1.0469e-01, -3.4276e-02,  3.5204e-02, -4.6629e-02,  1.0094e-01,\n",
      "         -6.3040e-02, -1.0441e-02,  1.4102e-02, -7.3728e-02, -9.7545e-02,\n",
      "          1.4607e-02,  1.0602e-01,  3.7415e-02, -6.6485e-02, -3.7701e-03,\n",
      "         -4.3467e-02,  9.2171e-03,  4.3062e-02,  5.6481e-02, -8.9433e-04,\n",
      "         -3.2676e-03, -1.0145e-01,  1.6765e-02, -5.8058e-02,  1.6679e-02,\n",
      "         -4.8842e-02,  9.0950e-02,  6.9278e-03, -7.4133e-02, -1.5954e-02,\n",
      "          1.0645e-01,  1.8322e-02, -9.3462e-02, -1.0705e-01,  2.8088e-02,\n",
      "         -6.3336e-02,  6.5605e-02,  1.0004e-01, -1.0261e-01,  4.8898e-02,\n",
      "         -1.0551e-01, -1.8998e-02, -1.0893e-01, -6.3354e-02],\n",
      "        [-9.0362e-02, -1.9945e-02, -5.0825e-02,  3.0411e-02, -6.5072e-02,\n",
      "         -1.0456e-01, -7.5965e-02, -1.1769e-02, -5.1239e-02, -4.6795e-02,\n",
      "         -3.7798e-02,  7.3686e-02,  5.3418e-03, -9.0606e-02,  3.6061e-02,\n",
      "          4.1710e-02, -2.0622e-04, -9.5983e-02, -9.3243e-02,  9.0252e-02,\n",
      "         -7.5973e-02, -6.0513e-02, -5.4521e-02,  1.0822e-01,  1.5134e-02,\n",
      "         -3.8531e-03,  9.7092e-02,  8.1424e-03, -6.9964e-02,  3.2413e-02,\n",
      "         -9.0520e-02, -1.6518e-02,  5.6036e-02,  4.7241e-02, -1.6752e-02,\n",
      "          4.0705e-02, -6.7748e-02, -7.1883e-02,  6.7800e-02,  2.2013e-02,\n",
      "         -1.2654e-02, -4.2182e-02,  2.0633e-02,  8.4022e-02,  6.6197e-02,\n",
      "          9.0408e-02,  7.1293e-02, -9.8637e-03, -7.0848e-02,  6.1616e-02,\n",
      "         -7.0114e-02,  9.0017e-02,  6.0281e-02, -5.1052e-02, -4.1919e-02,\n",
      "         -9.1279e-02, -4.4875e-02, -2.0721e-02, -1.0278e-01,  2.0030e-02,\n",
      "          1.7260e-02,  1.7627e-02,  1.0471e-01,  1.9028e-02,  7.3499e-02,\n",
      "         -8.3315e-02,  5.9128e-02, -1.0743e-01, -9.0144e-02, -5.3699e-02,\n",
      "         -3.8767e-02, -1.7590e-02,  4.6546e-02, -1.0669e-01, -5.0922e-02,\n",
      "         -9.4557e-02, -5.8790e-02,  1.1343e-02,  3.0807e-02, -3.8233e-02,\n",
      "          1.0014e-01, -6.8462e-02, -2.7425e-02, -5.9172e-02]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       requires_grad=True)\n",
      "before Parameter containing:\n",
      "tensor([-0.1040, -0.0670, -0.0107, -0.0175, -0.1063,  0.0668,  0.0749, -0.0290,\n",
      "        -0.0521,  0.0009], requires_grad=True)\n",
      "after Parameter containing:\n",
      "tensor([-0.1037, -0.0681, -0.0088, -0.0170, -0.1066,  0.0672,  0.0730, -0.0282,\n",
      "        -0.0512, -0.0004], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    print(\"before\",f)\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "    print(\"after\",f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: ```torch.optim``` that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** -: Observe how gradient buffers had to be manually set to zero using ```optimizer.zero_grad()```. This is because gradients are accumulated as explained in Backprop section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
